{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copyright Netherlands eScience Center <br>\n",
    "** Function     : Predict the Spatial Sea Ice Concentration with BayesConvLSTM at weekly time scale** <br>\n",
    "** Author       : Yang Liu ** <br>\n",
    "** First Built  : 2020.03.02 ** <br>\n",
    "** Last Update  : 2020.03.06 ** <br>\n",
    "** Library      : Pytorth, Numpy, NetCDF4, os, iris, cartopy, dlacs, matplotlib **<br>\n",
    "Description     : This notebook serves to predict the Arctic sea ice using deep learning. The Bayesian Convolutional Long Short Time Memory neural network is used to deal with this spatial-temporal sequence problem. We use Pytorch as the deep learning framework. <br>\n",
    "<br>\n",
    "** Here we predict sea ice concentration with one extra relevant field from either ocean or atmosphere to test the predictor.** <br>\n",
    "\n",
    "Return Values   : Time series and figures <br>\n",
    "\n",
    "The regionalization adopted here follows that of the MASIE (Multisensor Analyzed Sea Ice Extent) product available from the National Snow and Ice Data Center:<br>\n",
    "https://nsidc.org/data/masie/browse_regions<br>\n",
    "It is given by paper J.Walsh et. al., 2019. Benchmark seasonal prediction skill estimates based on regional indices.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "import numbers\n",
    "\n",
    "# for data loading\n",
    "import os\n",
    "from netCDF4 import Dataset\n",
    "# for pre-processing and machine learning\n",
    "import numpy as np\n",
    "import sklearn\n",
    "#import scipy\n",
    "import torch\n",
    "import torch.nn.functional\n",
    "\n",
    "#sys.path.append(os.path.join('C:','Users','nosta','ML4Climate','Scripts','DLACs'))\n",
    "#sys.path.append(\"C:\\\\Users\\\\nosta\\\\ML4Climate\\\\Scripts\\\\DLACs\")\n",
    "sys.path.append(\"../\")\n",
    "import dlacs\n",
    "import dlacs.BayesConvLSTM\n",
    "import dlacs.preprocess\n",
    "import dlacs.function\n",
    "\n",
    "# for visualization\n",
    "import dlacs.visual\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import cm\n",
    "import iris # also helps with regriding\n",
    "import cartopy\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "# ignore all the DeprecationWarnings by pytorch\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The testing device is Dell Inspirion 5680 with Intel Core i7-8700 x64 CPU and Nvidia GTX 1060 6GB GPU.<br>\n",
    "Here is a benchmark about cpu v.s. gtx 1060 <br>\n",
    "https://www.analyticsindiamag.com/deep-learning-tensorflow-benchmark-intel-i5-4210u-vs-geforce-nvidia-1060-6gb/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "constant = {'g' : 9.80616,      # gravititional acceleration [m / s2]\n",
    "            'R' : 6371009,      # radius of the earth [m]\n",
    "            'cp': 1004.64,      # heat capacity of air [J/(Kg*K)]\n",
    "            'Lv': 2500000,      # Latent heat of vaporization [J/Kg]\n",
    "            'R_dry' : 286.9,    # gas constant of dry air [J/(kg*K)]\n",
    "            'R_vap' : 461.5,    # gas constant for water vapour [J/(kg*K)]\n",
    "            'rho' : 1026,       # sea water density [kg/m3]\n",
    "            }\n",
    "\n",
    "################################################################################# \n",
    "#########                           datapath                             ########\n",
    "#################################################################################\n",
    "# please specify data path\n",
    "datapath_ERAI = '/home/ESLT0068/WorkFlow/Core_Database_DeepLearn/ERA-Interim'\n",
    "#datapath_ERAI = 'H:\\\\Creator_Zone\\\\Core_Database_DeepLearn\\\\ERA-Interim'\n",
    "datapath_ORAS4 = '/home/ESLT0068/WorkFlow/Core_Database_DeepLearn/ORAS4'\n",
    "#datapath_ORAS4 = 'H:\\\\Creator_Zone\\\\Core_Database_DeepLearn\\\\ORAS4'\n",
    "datapath_ORAS4_mask = '/home/ESLT0068/WorkFlow/Core_Database_DeepLearn/ORAS4'\n",
    "#datapath_ORAS4_mask = 'H:\\\\Creator_Zone\\\\Core_Database_DeepLearn\\\\ORAS4'\n",
    "#datapath_PIOMASS = '/home/ESLT0068/WorkFlow/Core_Database_AMET_OMET_reanalysis/PIOMASS'\n",
    "#datapath_PIOMASS = 'H:\\\\Creator_Zone\\\\Core_Database_AMET_OMET_reanalysis\\\\PIOMASS'\n",
    "#datapath_clim_index = '/home/ESLT0068/WorkFlow/Core_Database_AMET_OMET_reanalysis/Climate_index'\n",
    "#datapath_clim_index = 'F:\\\\PhD_essential\\\\Core_Database_AMET_OMET_reanalysis\\\\Climate_index'\n",
    "output_path = '/home/ESLT0068/NLeSC/Computation_Modeling/ML4Climate/PredictArctic/BayesMaps'\n",
    "#output_path = 'C:\\\\Users\\\\nosta\\\\ML4Climate\\\\PredictArctic\\\\BayesMaps'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********************** get the key to the datasets *************************\n",
      "*********************** extract variables *************************\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    print ('*********************** get the key to the datasets *************************')\n",
    "    # weekly variables on ERAI grid\n",
    "    dataset_ERAI_fields_sic = Dataset(os.path.join(datapath_ERAI,\n",
    "                                      'sic_weekly_erai_1979_2017.nc'))\n",
    "#     dataset_ERAI_fields_slp = Dataset(os.path.join(datapath_ERAI,\n",
    "#                                       'slp_weekly_erai_1979_2017.nc'))\n",
    "#     dataset_ERAI_fields_t2m = Dataset(os.path.join(datapath_ERAI,\n",
    "#                                       't2m_weekly_erai_1979_2017.nc'))\n",
    "#     dataset_ERAI_fields_z500 = Dataset(os.path.join(datapath_ERAI,\n",
    "#                                        'z500_weekly_erai_1979_2017.nc'))\n",
    "#     dataset_ERAI_fields_z850 = Dataset(os.path.join(datapath_ERAI,\n",
    "#                                        'z850_weekly_erai_1979_2017.nc'))\n",
    "#     dataset_ERAI_fields_uv10m = Dataset(os.path.join(datapath_ERAI,\n",
    "#                                        'uv10m_weekly_erai_1979_2017.nc'))\n",
    "#     dataset_ERAI_fields_rad = Dataset(os.path.join(datapath_ERAI,\n",
    "#                                         'rad_flux_weekly_erai_1979_2017.nc'))\n",
    "    #dataset_PIOMASS_siv = Dataset(os.path.join(datapath_PIOMASS,\n",
    "    #                             'siv_monthly_PIOMASS_1979_2017.nc'))\n",
    "    # OHC interpolated on ERA-Interim grid\n",
    "    dataset_ORAS4_OHC = Dataset(os.path.join(datapath_ORAS4,\n",
    "                                'ohc_monthly_oras2erai_1978_2017.nc'))\n",
    "#     dataset_index = Dataset(os.path.join(datapath_clim_index,\n",
    "#                             'index_climate_monthly_regress_1950_2017.nc'))\n",
    "    #dataset_ERAI_fields_flux = Dataset(os.path.join(datapath_ERAI_fields,\n",
    "    #                                  'surface_erai_monthly_regress_1979_2017_radiation.nc'))\n",
    "    # mask\n",
    "    dataset_ORAS4_mask = Dataset(os.path.join(datapath_ORAS4_mask, 'mesh_mask.nc'))\n",
    "    print ('*********************** extract variables *************************')\n",
    "    #################################################################################\n",
    "    #########                        data gallery                           #########\n",
    "    #################################################################################\n",
    "    # we use time series from 1979 to 2016 (468 months in total)\n",
    "    # training data: 1979 - 2013\n",
    "    # validation: 2014 - 2016\n",
    "    # variables list:\n",
    "    # SIC (ERA-Interim) / SIV (PIOMASS) / SST (ERA-Interim) / ST (ERA-Interim) / OHC (ORAS4) / AO-NAO-AMO-NINO3.4 (NOAA)\n",
    "    # integrals from spatial fields cover the area from 20N - 90N (4D fields [year, month, lat, lon])\n",
    "    # *************************************************************************************** #\n",
    "    # SIC (ERA-Interim) - benckmark\n",
    "    SIC_ERAI = dataset_ERAI_fields_sic.variables['sic'][:-1,:,:,:] # 4D fields [year, week, lat, lon]\n",
    "    year_ERAI = dataset_ERAI_fields_sic.variables['year'][:-1]\n",
    "    week_ERAI = dataset_ERAI_fields_sic.variables['week'][:]\n",
    "    latitude_ERAI = dataset_ERAI_fields_sic.variables['latitude'][:]\n",
    "    longitude_ERAI = dataset_ERAI_fields_sic.variables['longitude'][:]\n",
    "    # T2M (ERA-Interim)\n",
    "#     T2M_ERAI = dataset_ERAI_fields_t2m.variables['t2m'][:-1,:,:,:] # 4D fields [year, week, lat, lon]\n",
    "#     year_ERAI_t2m = dataset_ERAI_fields_t2m.variables['year'][:-1]\n",
    "#     week_ERAI_t2m = dataset_ERAI_fields_t2m.variables['week'][:]\n",
    "#     latitude_ERAI_t2m = dataset_ERAI_fields_t2m.variables['latitude'][:]\n",
    "#     longitude_ERAI_t2m = dataset_ERAI_fields_t2m.variables['longitude'][:]\n",
    "    # SLP (ERA-Interim)\n",
    "#     SLP_ERAI = dataset_ERAI_fields_slp.variables['slp'][:-1,:,:,:] # 4D fields [year, week, lat, lon]\n",
    "#     year_ERAI_slp = dataset_ERAI_fields_slp.variables['year'][:-1]\n",
    "#     week_ERAI_slp = dataset_ERAI_fields_slp.variables['week'][:]\n",
    "#     latitude_ERAI_slp = dataset_ERAI_fields_slp.variables['latitude'][:]\n",
    "#     longitude_ERAI_slp = dataset_ERAI_fields_slp.variables['longitude'][:]\n",
    "    # Z500 (ERA-Interim)\n",
    "#     Z500_ERAI = dataset_ERAI_fields_z500.variables['z'][:-1,:,:,:] # 4D fields [year, week, lat, lon]\n",
    "#     year_ERAI_z500 = dataset_ERAI_fields_z500.variables['year'][:-1]\n",
    "#     week_ERAI_z500 = dataset_ERAI_fields_z500.variables['week'][:]\n",
    "#     latitude_ERAI_z500 = dataset_ERAI_fields_z500.variables['latitude'][:]\n",
    "#     longitude_ERAI_z500 = dataset_ERAI_fields_z500.variables['longitude'][:]\n",
    "    # Z850 (ERA-Interim)\n",
    "#     Z850_ERAI = dataset_ERAI_fields_z850.variables['z'][:-1,:,:,:] # 4D fields [year, week, lat, lon]\n",
    "#     year_ERAI_z850 = dataset_ERAI_fields_z850.variables['year'][:-1]\n",
    "#     week_ERAI_z850 = dataset_ERAI_fields_z850.variables['week'][:]\n",
    "#     latitude_ERAI_z850 = dataset_ERAI_fields_z850.variables['latitude'][:]\n",
    "#     longitude_ERAI_z850 = dataset_ERAI_fields_z850.variables['longitude'][:]\n",
    "    # UV10M (ERA-Interim)\n",
    "#     U10M_ERAI = dataset_ERAI_fields_uv10m.variables['u10m'][:-1,:,:,:] # 4D fields [year, week, lat, lon]\n",
    "#     V10M_ERAI = dataset_ERAI_fields_uv10m.variables['v10m'][:-1,:,:,:] # 4D fields [year, week, lat, lon]\n",
    "#     year_ERAI_uv10m = dataset_ERAI_fields_uv10m.variables['year'][:-1]\n",
    "#     week_ERAI_uv10m = dataset_ERAI_fields_uv10m.variables['week'][:]\n",
    "#     latitude_ERAI_uv10m = dataset_ERAI_fields_uv10m.variables['latitude'][:]\n",
    "#     longitude_ERAI_uv10m = dataset_ERAI_fields_uv10m.variables['longitude'][:]\n",
    "    # SFlux (ERA-Interim)\n",
    "#     SFlux_ERAI = dataset_ERAI_fields_rad.variables['SFlux'][:-1,:,:,:] # 4D fields [year, week, lat, lon]\n",
    "#     year_ERAI_SFlux = dataset_ERAI_fields_rad.variables['year'][:-1]\n",
    "#     week_ERAI_SFlux = dataset_ERAI_fields_rad.variables['week'][:]\n",
    "#     latitude_ERAI_SFlux = dataset_ERAI_fields_rad.variables['latitude'][:]\n",
    "#     longitude_ERAI_SFlux = dataset_ERAI_fields_rad.variables['longitude'][:]\n",
    "    #SIV (PIOMASS)\n",
    "    #SIV_PIOMASS = dataset_PIOMASS_siv.variables['SIV'][:-12]\n",
    "    #year_SIV = dataset_PIOMASS_siv.variables['year'][:-1]\n",
    "    # OHC (ORAS4)\n",
    "    # from 1978 - 2017 (for interpolation) / from 90 N upto 40 N\n",
    "    OHC_300_ORAS4 = dataset_ORAS4_OHC.variables['OHC'][:-1,:,:67,:]/1000 # unit Peta Joule\n",
    "    latitude_ORAS4 = dataset_ORAS4_OHC.variables['latitude'][:]\n",
    "    longitude_ORAS4 = dataset_ORAS4_OHC.variables['longitude'][:]\n",
    "    mask_OHC = np.ma.getmask(OHC_300_ORAS4[0,0,:,:])\n",
    "    # AO-NAO-AMO-NINO3.4 (NOAA)\n",
    "#     AO = dataset_index.variables['AO'][348:-1] # from 1979 - 2017\n",
    "#     NAO = dataset_index.variables['NAO'][348:-1]\n",
    "#     NINO = dataset_index.variables['NINO'][348:-1]\n",
    "#     AMO = dataset_index.variables['AMO'][348:-1]\n",
    "#     PDO = dataset_index.variables['PDO'][348:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********************** create mask *************************\n",
      "*********************** calc mask *************************\n",
      "*********************** packing *************************\n",
      "******************  calculate extent from spatial fields  *******************\n",
      "================  reshape input data into time series  =================\n",
      "******************  choose the fields from target region  *******************\n",
      "******************  choose the fields from target region  *******************\n",
      "(1824, 24, 56)\n",
      "(1824, 24, 56)\n",
      "[80.5  79.75 79.   78.25 77.5  76.75 76.   75.25 74.5  73.75 73.   72.25\n",
      " 71.5  70.75 70.   69.25 68.5  67.75 67.   66.25 65.5  64.75 64.   63.25]\n",
      "[18.   18.75 19.5  20.25 21.   21.75 22.5  23.25 24.   24.75 25.5  26.25\n",
      " 27.   27.75 28.5  29.25 30.   30.75 31.5  32.25 33.   33.75 34.5  35.25\n",
      " 36.   36.75 37.5  38.25 39.   39.75 40.5  41.25 42.   42.75 43.5  44.25\n",
      " 45.   45.75 46.5  47.25 48.   48.75 49.5  50.25 51.   51.75 52.5  53.25\n",
      " 54.   54.75 55.5  56.25 57.   57.75 58.5  59.25]\n",
      "[80.5  79.75 79.   78.25 77.5  76.75 76.   75.25 74.5  73.75 73.   72.25\n",
      " 71.5  70.75 70.   69.25 68.5  67.75 67.   66.25 65.5  64.75 64.   63.25]\n",
      "[18.   18.75 19.5  20.25 21.   21.75 22.5  23.25 24.   24.75 25.5  26.25\n",
      " 27.   27.75 28.5  29.25 30.   30.75 31.5  32.25 33.   33.75 34.5  35.25\n",
      " 36.   36.75 37.5  38.25 39.   39.75 40.5  41.25 42.   42.75 43.5  44.25\n",
      " 45.   45.75 46.5  47.25 48.   48.75 49.5  50.25 51.   51.75 52.5  53.25\n",
      " 54.   54.75 55.5  56.25 57.   57.75 58.5  59.25]\n",
      "*******************  pre-processing  *********************\n",
      "=========================   normalize data   ===========================\n",
      "================  save the normalizing factor  =================\n",
      "1565.2049481856002 km2\n",
      "0.0 km2\n",
      "====================    A series of time (index)    ====================\n",
      "===================  artificial data for evaluation ====================\n"
     ]
    }
   ],
   "source": [
    "    #################################################################################\n",
    "    ###########                 global land-sea mask                      ###########\n",
    "    #################################################################################\n",
    "    sea_ice_mask_global = np.ones((len(latitude_ERAI),len(longitude_ERAI)),dtype=float)\n",
    "    sea_ice_mask_global[SIC_ERAI[0,0,:,:]==-1] = 0\n",
    "    #################################################################################\n",
    "    ###########                regionalization sea mask                   ###########\n",
    "    #################################################################################\n",
    "    print ('*********************** create mask *************************')\n",
    "    # W:-156 E:-124 N:80 S:67\n",
    "    mask_Beaufort = np.zeros((len(latitude_ERAI),len(longitude_ERAI)),dtype=int)\n",
    "    # W:-180 E:-156 N:80 S:66\n",
    "    mask_Chukchi = np.zeros((len(latitude_ERAI),len(longitude_ERAI)),dtype=int)\n",
    "    # W:146 E:180 N:80 S:67\n",
    "    mask_EastSiberian = np.zeros((len(latitude_ERAI),len(longitude_ERAI)),dtype=int)\n",
    "    # W:100 E:146 N:80 S:67\n",
    "    mask_Laptev = np.zeros((len(latitude_ERAI),len(longitude_ERAI)),dtype=int)\n",
    "    # W:60 E:100 N:80 S:67\n",
    "    mask_Kara = np.zeros((len(latitude_ERAI),len(longitude_ERAI)),dtype=int)\n",
    "    # W:18 E:60 N:80 S:64\n",
    "    mask_Barents = np.zeros((len(latitude_ERAI),len(longitude_ERAI)),dtype=int)\n",
    "    # W:-44 E:18 N:80 S:55\n",
    "    mask_Greenland = np.zeros((len(latitude_ERAI),len(longitude_ERAI)),dtype=int)\n",
    "    # W:-180 E:180 N:90 S:80\n",
    "    mask_CenArctic = np.zeros((len(latitude_ERAI),len(longitude_ERAI)),dtype=int)\n",
    "    print ('*********************** calc mask *************************')\n",
    "    mask_Beaufort[13:31,32:76] = 1\n",
    "\n",
    "    mask_Chukchi[13:32,0:32] = 1\n",
    "    mask_Chukchi[13:32,-1] = 1\n",
    "\n",
    "    mask_EastSiberian[13:31,434:479] = 1\n",
    "\n",
    "    mask_Laptev[13:31,374:434] = 1\n",
    "\n",
    "    mask_Kara[13:31,320:374] = 1\n",
    "\n",
    "    mask_Barents[13:36,264:320] = 1\n",
    "\n",
    "    mask_Greenland[13:47,179:264] = 1\n",
    "    mask_Greenland[26:47,240:264] = 0\n",
    "\n",
    "    mask_CenArctic[:13,:] = 1\n",
    "    print ('*********************** packing *************************')\n",
    "    mask_dict = {'Beaufort': mask_Beaufort[:,:],\n",
    "                 'Chukchi': mask_Chukchi[:,:],\n",
    "                 'EastSiberian': mask_EastSiberian[:,:],\n",
    "                 'Laptev': mask_Laptev[:,:],\n",
    "                 'Kara': mask_Kara[:,:],\n",
    "                 'Barents': mask_Barents[:,:],\n",
    "                 'Greenland': mask_Greenland[:,:],\n",
    "                 'CenArctic': mask_CenArctic[:,:]}\n",
    "    seas_namelist = ['Beaufort','Chukchi','EastSiberian','Laptev',\n",
    "                     'Kara', 'Barents', 'Greenland','CenArctic']\n",
    "    #################################################################################\n",
    "    ########                  temporal interpolation matrix                  ########\n",
    "    #################################################################################\n",
    "    # interpolate from monthly to weekly\n",
    "    # original monthly data will be taken as the last week of the month\n",
    "    OHC_300_ORAS4_weekly_series = np.zeros(SIC_ERAI.reshape(len(year_ERAI)*48,len(latitude_ERAI),len(longitude_ERAI)).shape,\n",
    "                                           dtype=float)\n",
    "    OHC_300_ORAS4_series= dlacs.preprocess.operator.unfold(OHC_300_ORAS4)\n",
    "    # calculate the difference between two months\n",
    "    OHC_300_ORAS4_deviation_series = (OHC_300_ORAS4_series[1:,:,:] - OHC_300_ORAS4_series[:-1,:,:]) / 4\n",
    "    for i in np.arange(4):\n",
    "        OHC_300_ORAS4_weekly_series[3-i::4,:,:] = OHC_300_ORAS4_series[12:,:,:] - i * OHC_300_ORAS4_deviation_series[11:,:,:]\n",
    "\n",
    "    print ('******************  calculate extent from spatial fields  *******************')\n",
    "    # size of the grid box\n",
    "    dx = 2 * np.pi * constant['R'] * np.cos(2 * np.pi * latitude_ERAI /\n",
    "                                            360) / len(longitude_ERAI)\n",
    "    dy = np.pi * constant['R'] / 480\n",
    "    # calculate the sea ice area\n",
    "    SIC_ERAI_area = np.zeros(SIC_ERAI.shape, dtype=float)\n",
    "#     SFlux_ERAI_area = np.zeros(SFlux_ERAI.shape, dtype=float)\n",
    "    for i in np.arange(len(latitude_ERAI[:])):\n",
    "        # change the unit to terawatt\n",
    "        SIC_ERAI_area[:,:,i,:] = SIC_ERAI[:,:,i,:]* dx[i] * dy / 1E+6 # unit km2\n",
    "#         SFlux_ERAI_area[:,:,i,:] = SFlux_ERAI[:,:,i,:]* dx[i] * dy / 1E+12 # unit TeraWatt\n",
    "    SIC_ERAI_area[SIC_ERAI_area<0] = 0 # switch the mask from -1 to 0\n",
    "    print ('================  reshape input data into time series  =================')\n",
    "    SIC_ERAI_area_series = dlacs.preprocess.operator.unfold(SIC_ERAI_area)\n",
    "#     T2M_ERAI_series = dlacs.preprocess.operator.unfold(T2M_ERAI)\n",
    "#     SLP_ERAI_series = dlacs.preprocess.operator.unfold(SLP_ERAI)\n",
    "#     Z500_ERAI_series = dlacs.preprocess.operator.unfold(Z500_ERAI)\n",
    "#     Z850_ERAI_series = dlacs.preprocess.operator.unfold(Z850_ERAI)\n",
    "#     U10M_ERAI_series = dlacs.preprocess.operator.unfold(U10M_ERAI)\n",
    "#     V10M_ERAI_series = dlacs.preprocess.operator.unfold(V10M_ERAI)\n",
    "#     SFlux_ERAI_area_series = dlacs.preprocess.operator.unfold(SFlux_ERAI_area)\n",
    "    print ('******************  choose the fields from target region  *******************')\n",
    "    # select land-sea mask\n",
    "    sea_ice_mask_barents = sea_ice_mask_global[12:36,264:320]\n",
    "    print ('******************  choose the fields from target region  *******************')\n",
    "    # select the area between greenland and ice land for instance 60-70 N / 44-18 W\n",
    "    sic_exp = SIC_ERAI_area_series[:,12:36,264:320]\n",
    "#     t2m_exp = T2M_ERAI_series[:,12:36,264:320]\n",
    "#     slp_exp = SLP_ERAI_series[:,12:36,264:320]\n",
    "#     z500_exp = Z500_ERAI_series[:,12:36,264:320]\n",
    "#     z850_exp = Z850_ERAI_series[:,12:36,264:320]\n",
    "#     u10m_exp = U10M_ERAI_series[:,12:36,264:320]\n",
    "#     v10m_exp = V10M_ERAI_series[:,12:36,264:320]\n",
    "#     sflux_exp = SFlux_ERAI_area_series[:,12:36,264:320]\n",
    "    ohc_exp = OHC_300_ORAS4_weekly_series[:,12:36,264:320]\n",
    "    print(sic_exp.shape)\n",
    "#     print(t2m_exp.shape)\n",
    "#     print(slp_exp.shape)\n",
    "#     print(z500_exp.shape)\n",
    "#     print(u10m_exp.shape)\n",
    "#     print(v10m_exp.shape)\n",
    "#     print(sflux_exp.shape)\n",
    "    print(ohc_exp.shape)\n",
    "    print(latitude_ERAI[12:36])\n",
    "    print(longitude_ERAI[264:320])\n",
    "    print(latitude_ORAS4[12:36])\n",
    "    print(longitude_ORAS4[264:320])\n",
    "    #print(latitude_ERAI[26:40])\n",
    "    #print(longitude_ERAI[180:216])\n",
    "    #print(sic_exp[:])\n",
    "    print ('*******************  pre-processing  *********************')\n",
    "    print ('=========================   normalize data   ===========================')\n",
    "    sic_exp_norm = dlacs.preprocess.operator.normalize(sic_exp)\n",
    "#     t2m_exp_norm = deepclim.preprocess.operator.normalize(t2m_exp)\n",
    "#     slp_exp_norm = deepclim.preprocess.operator.normalize(slp_exp)\n",
    "#     z500_exp_norm = deepclim.preprocess.operator.normalize(z500_exp)\n",
    "#     z850_exp_norm = deepclim.preprocess.operator.normalize(z850_exp)\n",
    "#     u10m_exp_norm = deepclim.preprocess.operator.normalize(u10m_exp)\n",
    "#     v10m_exp_norm = deepclim.preprocess.operator.normalize(v10m_exp)\n",
    "#     sflux_exp_norm = deepclim.preprocess.operator.normalize(sflux_exp)\n",
    "    ohc_exp_norm = dlacs.preprocess.operator.normalize(ohc_exp)\n",
    "    print('================  save the normalizing factor  =================')\n",
    "    sic_max = np.amax(sic_exp)\n",
    "    sic_min = np.amin(sic_exp)\n",
    "    print(sic_max,\"km2\")\n",
    "    print(sic_min,\"km2\")\n",
    "    print ('====================    A series of time (index)    ====================')\n",
    "    _, yy, xx = sic_exp_norm.shape # get the lat lon dimension\n",
    "    year = np.arange(1979,2017,1)\n",
    "    year_cycle = np.repeat(year,48)\n",
    "    month_cycle = np.repeat(np.arange(1,13,1),4)\n",
    "    month_cycle = np.tile(month_cycle,len(year)+1) # one extra repeat for lead time dependent prediction\n",
    "    month_cycle.astype(float)\n",
    "    month_2D = np.repeat(month_cycle[:,np.newaxis],yy,1)\n",
    "    month_exp = np.repeat(month_2D[:,:,np.newaxis],xx,2)\n",
    "    print ('===================  artificial data for evaluation ====================')\n",
    "    # calculate climatology of SIC\n",
    "#     seansonal_cycle_SIC = np.zeros(48,dtype=float)\n",
    "#     for i in np.arange(48):\n",
    "#         seansonal_cycle_SIC[i] = np.mean(SIC_ERAI_sum_norm[i::48],axis=0)\n",
    "    # weight for loss\n",
    "#     weight_month = np.array([0,1,1,\n",
    "#                              1,0,0,\n",
    "#                              1,1,1,\n",
    "#                              0,0,0])\n",
    "    #weight_loss = np.repeat(weight_month,4)\n",
    "    #weight_loss = np.tile(weight_loss,len(year))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procedure for LSTM <br>\n",
    "** We use Pytorth to implement LSTM neural network with time series of climate data. ** <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************  parameter for check  *********************\n",
      "*******************  create basic dimensions for tensor and network  *********************\n",
      "*******************  cross validation and testing data  *********************\n",
      "*******************  check the environment  *********************\n",
      "Pytorch version 1.1.0\n",
      "Is CUDA available? False\n",
      "*******************  run BayesConvLSTM  *********************\n",
      "The model is designed to make many to one prediction.\n",
      "A series of multi-chanel variables will be input to the model.\n",
      "The model learns by verifying the output at each timestep.\n",
      "BayesConvLSTM(\n",
      "  (cell0): BayesConvLSTMCell()\n",
      "  (cell1): BayesConvLSTMCell()\n",
      "  (cell2): BayesConvLSTMCell()\n",
      ")\n",
      "ELBO(\n",
      "  (loss_function): KLDivLoss()\n",
      ")\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.005\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "    print ('*******************  parameter for check  *********************')\n",
    "    choice_exp_norm = ohc_exp_norm\n",
    "    print ('*******************  create basic dimensions for tensor and network  *********************')\n",
    "    # specifications of neural network\n",
    "    input_channels = 3\n",
    "    hidden_channels = [3, 2, 1] # number of channels & hidden layers, the channels of last layer is the channels of output, too\n",
    "    #hidden_channels = [3, 3, 3, 3, 2]\n",
    "    #hidden_channels = [2]\n",
    "    kernel_size = 3\n",
    "    # here we input a sequence and predict the next step only\n",
    "    #step = 1 # how many steps to predict ahead\n",
    "    #effective_step = [0] # step to output\n",
    "    batch_size = 1\n",
    "    #num_layers = 1\n",
    "    learning_rate = 0.005\n",
    "    num_epochs = 15#00\n",
    "    print ('*******************  cross validation and testing data  *********************')\n",
    "    # take 10% data as cross-validation data\n",
    "    cross_valid_year = 4\n",
    "    # take 10% years as testing data\n",
    "    test_year = 4\n",
    "    # minibatch\n",
    "    #iterations = 3 # training data divided into 3 sets\n",
    "    print ('*******************  check the environment  *********************')\n",
    "    print (\"Pytorch version {}\".format(torch.__version__))\n",
    "    # check if CUDA is available\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    print(\"Is CUDA available? {}\".format(use_cuda))\n",
    "    # CUDA settings torch.__version__ must > 0.4\n",
    "    # !!! This is important for the model!!! The first option is gpu\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")   \n",
    "    print ('*******************  run BayesConvLSTM  *********************')\n",
    "    print ('The model is designed to make many to one prediction.')\n",
    "    print ('A series of multi-chanel variables will be input to the model.')\n",
    "    print ('The model learns by verifying the output at each timestep.')\n",
    "    # check the sequence length\n",
    "    sequence_len, height, width = sic_exp_norm.shape\n",
    "    # initialize our model\n",
    "    model = dlacs.BayesConvLSTM.BayesConvLSTM(input_channels, hidden_channels, kernel_size).to(device)\n",
    "    # use Evidence Lower Bound (ELBO) to quantify the loss\n",
    "    ELBO = dlacs.function.ELBO(height*width)\n",
    "    # for classification, target must be integers (label)\n",
    "    #ELBO = dlacs.function.ELBO(height*width,loss_function=nn.CrossEntropyLoss(reduction='mean'))\n",
    "    #ELBO = dlacs.function.ELBO(height*width,loss_function=nn.NLLLoss(reduction='mean'))\n",
    "    # stochastic gradient descent\n",
    "    #optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    # Adam optimizer\n",
    "    optimiser = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    print(model)\n",
    "    print(ELBO)\n",
    "    print(optimiser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print (name)\n",
    "            print (param.data)\n",
    "            print (param.size())\n",
    "            print (\"=========================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################################################\n",
      "#############  preview model parameters matrix  ###############\n",
      "##############################################################\n",
      "Number of parameter matrices:  60\n",
      "torch.Size([3, 3, 3, 3])\n",
      "torch.Size([3, 3, 3, 3])\n",
      "torch.Size([3, 3, 3, 3])\n",
      "torch.Size([3, 3, 3, 3])\n",
      "torch.Size([3, 3, 3, 3])\n",
      "torch.Size([3, 3, 3, 3])\n",
      "torch.Size([3, 3, 3, 3])\n",
      "torch.Size([3, 3, 3, 3])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([2, 3, 3, 3])\n",
      "torch.Size([2, 3, 3, 3])\n",
      "torch.Size([2, 3, 3, 3])\n",
      "torch.Size([2, 3, 3, 3])\n",
      "torch.Size([2, 3, 3, 3])\n",
      "torch.Size([2, 3, 3, 3])\n",
      "torch.Size([2, 3, 3, 3])\n",
      "torch.Size([2, 3, 3, 3])\n",
      "torch.Size([2])\n",
      "torch.Size([2])\n",
      "torch.Size([2])\n",
      "torch.Size([2])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 2, 3, 3])\n",
      "torch.Size([1, 2, 3, 3])\n",
      "torch.Size([1, 2, 3, 3])\n",
      "torch.Size([1, 2, 3, 3])\n",
      "torch.Size([1, 2, 3, 3])\n",
      "torch.Size([1, 2, 3, 3])\n",
      "torch.Size([1, 2, 3, 3])\n",
      "torch.Size([1, 2, 3, 3])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "    print('##############################################################')\n",
    "    print('#############  preview model parameters matrix  ###############')\n",
    "    print('##############################################################')\n",
    "    print('Number of parameter matrices: ', len(list(model.parameters())))\n",
    "    for i in range(len(list(model.parameters()))):\n",
    "        print(list(model.parameters())[i].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################################################\n",
      "##################  start training loop  #####################\n",
      "##############################################################\n",
      "Initialization layer 0\n",
      "Initialization layer 1\n",
      "Initialization layer 2\n",
      "Epoch  0 MSE:  2824936192.0\n",
      "d\n",
      "a\n",
      "b\n",
      "c\n",
      "Initialization layer 0\n",
      "Initialization layer 1\n",
      "Initialization layer 2\n",
      "d\n",
      "a\n",
      "b\n",
      "c\n",
      "Initialization layer 0\n",
      "Initialization layer 1\n",
      "Initialization layer 2\n",
      "d\n",
      "a\n",
      "b\n",
      "c\n",
      "Initialization layer 0\n",
      "Initialization layer 1\n",
      "Initialization layer 2\n",
      "d\n",
      "a\n",
      "b\n",
      "c\n",
      "Initialization layer 0\n",
      "Initialization layer 1\n",
      "Initialization layer 2\n",
      "d\n",
      "a\n",
      "b\n",
      "c\n",
      "Initialization layer 0\n",
      "Initialization layer 1\n",
      "Initialization layer 2\n",
      "Epoch  5 MSE:  1354228864.0\n",
      "d\n",
      "a\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.2.0/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.2.0/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "    %%time\n",
    "    print('##############################################################')\n",
    "    print('##################  start training loop  #####################')\n",
    "    print('##############################################################')\n",
    "    # track training loss\n",
    "    hist = np.zeros(num_epochs)\n",
    "    # loop of epoch\n",
    "    for t in range(num_epochs):\n",
    "        # Clear stored gradient\n",
    "        model.zero_grad()\n",
    "        # loop of timestep\n",
    "        for timestep in range(sequence_len - cross_valid_year*12*4 - test_year*12*4):\n",
    "            # hidden state re-initialized inside the model when timestep=0\n",
    "            #################################################################################\n",
    "            ########          create input tensor with multi-input dimension         ########\n",
    "            #################################################################################\n",
    "            # create variables\n",
    "            x_input = np.stack((sic_exp_norm[timestep,:,:],\n",
    "                                choice_exp_norm[timestep,:,:],\n",
    "                                month_exp[timestep,:,:])) #vstack,hstack,dstack\n",
    "            x_var = torch.autograd.Variable(torch.Tensor(x_input).view(-1,input_channels,height,width)).to(device)\n",
    "            #################################################################################\n",
    "            ########       create training tensor with multi-input dimension         ########\n",
    "            #################################################################################\n",
    "            y_train_stack = sic_exp_norm[timestep+1,:,:] #vstack,hstack,dstack\n",
    "            y_var = torch.autograd.Variable(torch.Tensor(y_train_stack).view(-1,hidden_channels[-1],height,width)).to(device)\n",
    "            #################################################################################   \n",
    "            # Forward pass\n",
    "            y_pred, kl_loss, _ = model(x_var, timestep)\n",
    "            # choose training data\n",
    "            y_target = y_var\n",
    "            # torch.nn.functional.mse_loss(y_pred, y_train) can work with (scalar,vector) & (vector,vector)\n",
    "            # Please Make Sure y_pred & y_train have the same dimension\n",
    "            # accumulate loss\n",
    "            if timestep == 0:\n",
    "                loss = ELBO(y_pred, y_target, kl_loss)\n",
    "            else:\n",
    "                loss += ELBO(y_pred, y_target, kl_loss)\n",
    "            #print (timestep)\n",
    "        #print(y_pred.shape)\n",
    "        #print(y_train.shape)\n",
    "        # print loss at certain iteration\n",
    "        if t % 5 == 0:\n",
    "            print(\"Epoch \", t, \"MSE: \", loss.item())\n",
    "            #print(y_pred)\n",
    "            # gradient check\n",
    "            # Gradcheck requires double precision numbers to run\n",
    "            #res = torch.autograd.gradcheck(loss_fn, (y_pred.double(), y_train.double()), eps=1e-6, raise_exception=True)\n",
    "            #print(res)\n",
    "        hist[t] = loss.item()\n",
    "\n",
    "        # Zero out gradient, else they will accumulate between epochs\n",
    "        optimiser.zero_grad()\n",
    "    \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update parameters\n",
    "        optimiser.step()\n",
    "        \n",
    "    # save the model\n",
    "    # (recommended) save the model parameters only\n",
    "    torch.save(model.state_dict(), os.path.join(output_path,'bayesconvlstm.pkl'))\n",
    "    # save the entire model\n",
    "    # torch.save(model, os.path.join(output_path,'bayesconvlstm.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************  Loss with time  **********************\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "    print (\"*******************  Loss with time  **********************\")\n",
    "    fig00 = plt.figure()\n",
    "    try:\n",
    "        plt.plot(hist, label=\"Training loss\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        fig00.savefig(os.path.join(output_path,'SIC_ERAI_LSTM_pred_error.png'),dpi=200)\n",
    "    except:\n",
    "        print('Model is reloaded instead of trained!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************  evaluation matrix  *********************\n"
     ]
    }
   ],
   "source": [
    "    print ('*******************  evaluation matrix  *********************')\n",
    "    # The prediction will be evaluated through RMSE against climatology\n",
    "    \n",
    "    # error score for temporal-spatial fields, without keeping spatial pattern\n",
    "    def RMSE(x,y):\n",
    "        \"\"\"\n",
    "        Calculate the RMSE. x is input series and y is reference series.\n",
    "        It calculates RMSE over the domain, not over time. The spatial structure\n",
    "        will not be kept.\n",
    "        Parameter\n",
    "        ----------------------\n",
    "        x: input time series with the shape [time, lat, lon]\n",
    "        \"\"\"\n",
    "        x_series = x.reshape(x.shape[0],-1)\n",
    "        y_series = y.reshape(y.shape[0],-1)\n",
    "        rmse = np.sqrt(np.mean((x_series - y_series)**2,1))\n",
    "        rmse_std = np.sqrt(np.std((x_series - y_series)**2,1))\n",
    "    \n",
    "        return rmse, rmse_std\n",
    "    \n",
    "    # error score for temporal-spatial fields, keeping spatial pattern\n",
    "    def MAE(x,y):\n",
    "        \"\"\"\n",
    "        Calculate the MAE. x is input series and y is reference series.\n",
    "        It calculate MAE over time and keeps the spatial structure.\n",
    "        \"\"\"\n",
    "        mae = np.mean(np.abs(x-y),0)\n",
    "        \n",
    "        return mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################################################\n",
      "###################  start prediction loop ###################\n",
      "##############################################################\n",
      "*******************************  one step ahead forecast  *********************************\n",
      "************  the last 4 years of total time series are treated as test data  ************\n",
      "Wall time: 34min 18s\n"
     ]
    }
   ],
   "source": [
    "    %%time\n",
    "    #################################################################################\n",
    "    ########  operational lead time dependent prediction with testing data   ########\n",
    "    #################################################################################\n",
    "    print('##############################################################')\n",
    "    print('###################  start prediction loop ###################')\n",
    "    print('##############################################################')\n",
    "    # the model learn from time series and try to predict the next time step based on the previous time series\n",
    "    print ('*******************************  one step ahead forecast  *********************************')\n",
    "    print ('************  the last {} years of total time series are treated as test data  ************'.format(test_year))\n",
    "    # time series before test data\n",
    "    pred_base_sic = sic_exp_norm[:-test_year*12*4,:,:]\n",
    "    # predict x steps ahead\n",
    "    step_lead = 16 # unit week\n",
    "    # create a matrix for the prediction\n",
    "    lead_pred_sic = np.zeros((test_year*12*4,step_lead,height,width),dtype=float) # dim [predict time, lead time, lat, lon]\n",
    "    # start the prediction loop\n",
    "    for step in range(test_year*12*4):\n",
    "        # Clear stored gradient\n",
    "        model.zero_grad()\n",
    "        # Don't do this if you want your LSTM to be stateful\n",
    "        # Otherwise the hidden state should be cleaned up at each time step for prediction (we don't clear hidden state in our forward function)\n",
    "        # see example from (https://github.com/pytorch/examples/blob/master/time_sequence_prediction/train.py)\n",
    "        # model.hidden = model.init_hidden()\n",
    "        # based on the design of this module, the hidden states and cell states are initialized when the module is called.\n",
    "        for i in np.arange(1,sequence_len-test_year*12*4 + step + step_lead,1): # here i is actually the time step (index) of prediction, we use var[:i] to predict var[i]\n",
    "            #############################################################################\n",
    "            ###############           before time of prediction           ###############\n",
    "            #############################################################################\n",
    "            if i <= (sequence_len-test_year*12*4 + step):\n",
    "                # create variables\n",
    "                x_input = np.stack((sic_exp_norm[i-1,:,:],\n",
    "                                    choice_exp_norm[i-1,:,:],\n",
    "                                    month_exp[i-1,:,:])) #vstack,hstack,dstack\n",
    "                x_var_pred = torch.autograd.Variable(torch.Tensor(x_input).view(-1,input_channels,height,width),\n",
    "                                                     requires_grad=False).cuda()\n",
    "                # make prediction\n",
    "                last_pred, _ = model(x_var_pred, i-1, training=False)\n",
    "                # record the real prediction after the time of prediction\n",
    "                if i == (sequence_len-test_year*12*4 + step):\n",
    "                    lead = 0\n",
    "                    # GPU data should be transferred to CPU\n",
    "                    lead_pred_sic[step,0,:,:] = last_pred[0,0,:,:].cpu().data.numpy()\n",
    "            #############################################################################\n",
    "            ###############            after time of prediction           ###############\n",
    "            #############################################################################\n",
    "            else:\n",
    "                lead += 1\n",
    "                # prepare predictor\n",
    "                if i <= sequence_len:\n",
    "                    # use the predicted data to make new prediction\n",
    "                    x_input = np.stack((lead_pred_sic[step,i-(sequence_len-test_year*12*4 + step +1),:,:],\n",
    "                                        choice_exp_norm[i-1,:,:],\n",
    "                                        month_exp[i-1,:,:])) #vstack,hstack,dstack\n",
    "                else: # choice_exp_norm out of range, use the last value\n",
    "                    x_input = np.stack((lead_pred_sic[step,i-(sequence_len-test_year*12*4 + step +1),:,:],\n",
    "                                        choice_exp_norm[-1,:,:],\n",
    "                                        month_exp[i-1,:,:])) #vstack,hstack,dstack                    \n",
    "                x_var_pred = torch.autograd.Variable(torch.Tensor(x_input).view(-1,input_channels,height,width),\n",
    "                                                     requires_grad=False).cuda()        \n",
    "                # make prediction\n",
    "                last_pred, _ = model(x_var_pred, i-1, training=False)\n",
    "                # record the prediction\n",
    "                lead_pred_sic[step,lead,:,:] = last_pred[0,0,:,:].cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def week2month(series, m):\n",
    "    \"\"\"\n",
    "    Select certain month from yearly data at weekly resolution.\n",
    "    Parameters\n",
    "    ----------\n",
    "    series : array-like\n",
    "        Three-dimensional numeric arrays with time as the first dimenison [time, lat, lon]\n",
    "    m: int\n",
    "        Month (from 1 to 12).\n",
    "    \"\"\"\n",
    "    time_year, lat, lon = series.shape\n",
    "    time_month = time_year // 12\n",
    "    series_month = np.zeros((time_month, lat, lon), dtype=float)\n",
    "    series_month[::4,:,:] = series[(m-1)*4::48,:,:]\n",
    "    series_month[1::4,:,:] = series[(m-1)*4+1::48,:,:]\n",
    "    series_month[2::4,:,:] = series[(m-1)*4+2::48,:,:]\n",
    "    series_month[3::4,:,:] = series[(m-1)*4+3::48,:,:]\n",
    "    \n",
    "    return series_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################################################\n",
      "############   start prediction with climatology  ############\n",
      "##############################################################\n",
      "##############################################################\n",
      "############   start prediction with persistence  ############\n",
      "##############################################################\n",
      "##############################################################\n",
      "########   land-sea correction for sic prediction    #########\n",
      "##############################################################\n",
      "#################################################################################\n",
      "############   evaluation and statistical matrix for the entire year ############\n",
      "#################################################################################\n",
      "*******************     Lead time 0     *******************\n",
      "Mean RMSE with testing data - ConvLSTM\n",
      "50.706676528696136 +- 101.52656804735905\n",
      "Mean RMSE with testing data - Climatology\n",
      "137.91125915081744 +- 212.30182952186146\n",
      "Mean RMSE with testing data - Persistence\n",
      "50.170794466755474 +- 104.00027668956385\n",
      "*******************     Lead time 1     *******************\n",
      "Mean RMSE with testing data - ConvLSTM\n",
      "73.80067980850241 +- 143.49283429508566\n",
      "Mean RMSE with testing data - Persistence\n",
      "75.9458837476023 +- 153.19988772374379\n",
      "*******************     Lead time 2     *******************\n",
      "Mean RMSE with testing data - ConvLSTM\n",
      "87.97012328792476 +- 167.7852734894779\n",
      "Mean RMSE with testing data - Persistence\n",
      "93.91398900099325 +- 186.0275468269465\n",
      "*******************     Lead time 3     *******************\n",
      "Mean RMSE with testing data - ConvLSTM\n",
      "98.5188860402766 +- 185.6837423753026\n",
      "Mean RMSE with testing data - Persistence\n",
      "110.1283583502817 +- 213.094235734779\n",
      "*******************     Lead time 4     *******************\n",
      "Mean RMSE with testing data - ConvLSTM\n",
      "107.31380808774915 +- 198.38616487734134\n",
      "Mean RMSE with testing data - Persistence\n",
      "124.67600441157826 +- 234.89027045606795\n",
      "*******************     Lead time 5     *******************\n",
      "Mean RMSE with testing data - ConvLSTM\n",
      "114.11048065520377 +- 207.20630612685207\n",
      "Mean RMSE with testing data - Persistence\n",
      "138.19688180429785 +- 253.81677913415027\n",
      "*******************     Lead time 6     *******************\n",
      "Mean RMSE with testing data - ConvLSTM\n",
      "120.01947589345633 +- 214.58713240940858\n",
      "Mean RMSE with testing data - Persistence\n",
      "149.77746834844268 +- 270.6962810970926\n",
      "*******************     Lead time 7     *******************\n",
      "Mean RMSE with testing data - ConvLSTM\n",
      "124.84219094751715 +- 218.869082473999\n",
      "Mean RMSE with testing data - Persistence\n",
      "159.60545845774672 +- 284.95967429502997\n",
      "*******************     Lead time 8     *******************\n",
      "Mean RMSE with testing data - ConvLSTM\n",
      "128.7138520548639 +- 221.84704752394927\n",
      "Mean RMSE with testing data - Persistence\n",
      "169.0739008031674 +- 298.8606259213634\n",
      "*******************     Lead time 9     *******************\n",
      "Mean RMSE with testing data - ConvLSTM\n",
      "131.58849734108065 +- 222.60721890135395\n",
      "Mean RMSE with testing data - Persistence\n",
      "178.18772775645525 +- 312.3710063626479\n",
      "*******************     Lead time 10     *******************\n",
      "Mean RMSE with testing data - ConvLSTM\n",
      "134.20592838035523 +- 222.89458852948098\n",
      "Mean RMSE with testing data - Persistence\n",
      "186.4387677456621 +- 324.1197528679833\n",
      "*******************     Lead time 11     *******************\n",
      "Mean RMSE with testing data - ConvLSTM\n",
      "136.4485921157947 +- 222.887362112187\n",
      "Mean RMSE with testing data - Persistence\n",
      "193.67677658258384 +- 334.85034457777834\n",
      "*******************     Lead time 12     *******************\n",
      "Mean RMSE with testing data - ConvLSTM\n",
      "138.1939411275557 +- 221.5000900636044\n",
      "Mean RMSE with testing data - Persistence\n",
      "200.32998646154147 +- 343.7878046378051\n",
      "*******************     Lead time 13     *******************\n",
      "Mean RMSE with testing data - ConvLSTM\n",
      "139.60593380026867 +- 219.41595016377573\n",
      "Mean RMSE with testing data - Persistence\n",
      "206.51543766766818 +- 351.4327196584504\n",
      "*******************     Lead time 14     *******************\n",
      "Mean RMSE with testing data - ConvLSTM\n",
      "140.67190120656565 +- 217.0785990279794\n",
      "Mean RMSE with testing data - Persistence\n",
      "212.35298561146928 +- 358.3893891527145\n",
      "*******************     Lead time 15     *******************\n",
      "Mean RMSE with testing data - ConvLSTM\n",
      "142.5877510277676 +- 215.99804451496826\n",
      "Mean RMSE with testing data - Persistence\n",
      "218.3251307542433 +- 365.56173747786585\n",
      "##############################################################\n",
      "############           create a txt file          ############\n",
      "##############################################################\n",
      "*******************    1     *******************\n",
      "RMSE - ConvLSTM       90.03199503565503 + - 176.79836619229437\n",
      "RMSE - Climatology    183.7509720006571 + - 280.49158911076336\n",
      "RMSE - Persistence    91.5997479178463 + - 180.9916370668999\n",
      "*******************    2     *******************\n",
      "RMSE - ConvLSTM       85.42853607325918 + - 170.12721098131493\n",
      "RMSE - Climatology    183.30330023934812 + - 273.347176573773\n",
      "RMSE - Persistence    82.42039524259681 + - 160.9626777381962\n",
      "*******************    3     *******************\n",
      "RMSE - ConvLSTM       80.57466662639172 + - 148.43041863129613\n",
      "RMSE - Climatology    199.89157834353045 + - 304.6694702911925\n",
      "RMSE - Persistence    83.91081918286545 + - 157.80740953153088\n",
      "*******************    4     *******************\n",
      "RMSE - ConvLSTM       73.26888508374091 + - 139.66337783029985\n",
      "RMSE - Climatology    207.80614115696767 + - 313.8306190145979\n",
      "RMSE - Persistence    76.4423650624924 + - 151.35030443127886\n",
      "*******************    5     *******************\n",
      "RMSE - ConvLSTM       60.01419233058145 + - 114.76646554868893\n",
      "RMSE - Climatology    182.62746359795213 + - 261.6904610096677\n",
      "RMSE - Persistence    69.48336388255558 + - 138.76485236989456\n",
      "*******************    6     *******************\n",
      "RMSE - ConvLSTM       36.12961583718595 + - 83.99245611134056\n",
      "RMSE - Climatology    149.31103629366999 + - 224.27726748630144\n",
      "RMSE - Persistence    46.7494984784352 + - 108.87355954907032\n",
      "*******************    7     *******************\n",
      "RMSE - ConvLSTM       18.469664978555283 + - 44.83353200868966\n",
      "RMSE - Climatology    73.29991659481595 + - 129.15866781119783\n",
      "RMSE - Persistence    23.531599159186072 + - 60.06396625220598\n",
      "*******************    8     *******************\n",
      "RMSE - ConvLSTM       17.788191297756605 + - 42.45268822018319\n",
      "RMSE - Climatology    46.699229356606295 + - 95.94623801740428\n",
      "RMSE - Persistence    10.98501001991176 + - 29.099367694308427\n",
      "*******************    9     *******************\n",
      "RMSE - ConvLSTM       17.088487755091954 + - 35.9739502981632\n",
      "RMSE - Climatology    42.771801967839494 + - 84.80483251772198\n",
      "RMSE - Persistence    5.635869541447041 + - 14.67266230343604\n",
      "*******************    10     *******************\n",
      "RMSE - ConvLSTM       28.363484686519065 + - 53.370271119785706\n",
      "RMSE - Climatology    72.14880256647942 + - 122.6752667121632\n",
      "RMSE - Persistence    14.224962737838037 + - 31.34599103819741\n",
      "*******************    11     *******************\n",
      "RMSE - ConvLSTM       36.86205923514393 + - 78.88086771393324\n",
      "RMSE - Climatology    147.2286115015819 + - 210.92864385138375\n",
      "RMSE - Persistence    30.528658826634373 + - 76.59773465361295\n",
      "*******************    12     *******************\n",
      "RMSE - ConvLSTM       64.46033940447249 + - 129.02921191231889\n",
      "RMSE - Climatology    166.09625619036072 + - 245.80172186617085\n",
      "RMSE - Persistence    66.53724354925671 + - 137.47315764613458\n"
     ]
    }
   ],
   "source": [
    "    #################################################################################\n",
    "    ########                performance evaluation with RMSE                 ########\n",
    "    ########              RMSE over time, and sum over domain                ########\n",
    "    #################################################################################\n",
    "    sequence_len, height, width = sic_exp_norm.shape\n",
    "    print('##############################################################')\n",
    "    print('############   start prediction with climatology  ############')\n",
    "    print('##############################################################')\n",
    "    # compute climatology\n",
    "    climatology = np.zeros((48, height, width),dtype=float)\n",
    "    for i in range(48):\n",
    "        climatology[i,:,:] = np.mean(sic_exp_norm[i::48,:,:],axis=0)\n",
    "    # repeat this climatology and calculate the RMSE\n",
    "    climatology_seq = np.tile(climatology,(test_year,1,1))\n",
    "    RMSE_climatology, RMSE_climatology_std  = RMSE(climatology_seq * sic_max,sic_exp_norm[-test_year*12*4:,:,:] * sic_max)\n",
    "    RMSE_climatology = np.mean(RMSE_climatology)\n",
    "    RMSE_climatology_std = np.mean(RMSE_climatology_std)\n",
    "    print('##############################################################')\n",
    "    print('############   start prediction with persistence  ############')\n",
    "    print('##############################################################')\n",
    "    RMSE_persist_0, RMSE_persist_0_std = RMSE(sic_exp_norm[-test_year*12*4-1:-1,:,:] * sic_max,\n",
    "                                              sic_exp_norm[-test_year*12*4:,:,:] * sic_max)\n",
    "    RMSE_persist_1, RMSE_persist_1_std = RMSE(sic_exp_norm[-test_year*12*4-1:-2,:,:] * sic_max,\n",
    "                                              sic_exp_norm[-test_year*12*4+1:,:,:] * sic_max)\n",
    "    RMSE_persist_2, RMSE_persist_2_std = RMSE(sic_exp_norm[-test_year*12*4-1:-3,:,:] * sic_max,\n",
    "                                              sic_exp_norm[-test_year*12*4+2:,:,:] * sic_max)\n",
    "    RMSE_persist_3, RMSE_persist_3_std = RMSE(sic_exp_norm[-test_year*12*4-1:-4,:,:] * sic_max,\n",
    "                                              sic_exp_norm[-test_year*12*4+3:,:,:] * sic_max)\n",
    "    RMSE_persist_4, RMSE_persist_4_std = RMSE(sic_exp_norm[-test_year*12*4-1:-5,:,:] * sic_max,\n",
    "                                              sic_exp_norm[-test_year*12*4+4:,:,:] * sic_max)\n",
    "    RMSE_persist_5, RMSE_persist_5_std = RMSE(sic_exp_norm[-test_year*12*4-1:-6,:,:] * sic_max,\n",
    "                                              sic_exp_norm[-test_year*12*4+5:,:,:] * sic_max)\n",
    "    RMSE_persist_6, RMSE_persist_6_std = RMSE(sic_exp_norm[-test_year*12*4-1:-7,:,:] * sic_max,\n",
    "                                              sic_exp_norm[-test_year*12*4+6:,:,:] * sic_max)\n",
    "    RMSE_persist_7, RMSE_persist_7_std = RMSE(sic_exp_norm[-test_year*12*4-1:-8,:,:] * sic_max,\n",
    "                                              sic_exp_norm[-test_year*12*4+7:,:,:] * sic_max)\n",
    "    RMSE_persist_8, RMSE_persist_8_std = RMSE(sic_exp_norm[-test_year*12*4-1:-9,:,:] * sic_max,\n",
    "                                              sic_exp_norm[-test_year*12*4+8:,:,:] * sic_max)\n",
    "    RMSE_persist_9, RMSE_persist_9_std = RMSE(sic_exp_norm[-test_year*12*4-1:-10,:,:] * sic_max,\n",
    "                                              sic_exp_norm[-test_year*12*4+9:,:,:] * sic_max)\n",
    "    RMSE_persist_10, RMSE_persist_10_std = RMSE(sic_exp_norm[-test_year*12*4-1:-11,:,:] * sic_max,\n",
    "                                              sic_exp_norm[-test_year*12*4+10:,:,:] * sic_max)\n",
    "    RMSE_persist_11, RMSE_persist_11_std = RMSE(sic_exp_norm[-test_year*12*4-1:-12,:,:] * sic_max,\n",
    "                                              sic_exp_norm[-test_year*12*4+11:,:,:] * sic_max)\n",
    "    RMSE_persist_12, RMSE_persist_12_std = RMSE(sic_exp_norm[-test_year*12*4-1:-13,:,:] * sic_max,\n",
    "                                              sic_exp_norm[-test_year*12*4+12:,:,:] * sic_max)\n",
    "    RMSE_persist_13, RMSE_persist_13_std = RMSE(sic_exp_norm[-test_year*12*4-1:-14,:,:] * sic_max,\n",
    "                                              sic_exp_norm[-test_year*12*4+13:,:,:] * sic_max)\n",
    "    RMSE_persist_14, RMSE_persist_14_std = RMSE(sic_exp_norm[-test_year*12*4-1:-15,:,:] * sic_max,\n",
    "                                              sic_exp_norm[-test_year*12*4+14:,:,:] * sic_max)\n",
    "    RMSE_persist_15, RMSE_persist_15_std = RMSE(sic_exp_norm[-test_year*12*4-1:-16,:,:] * sic_max,\n",
    "                                              sic_exp_norm[-test_year*12*4+15:,:,:] * sic_max)\n",
    "    print('##############################################################')\n",
    "    print('########   land-sea correction for sic prediction    #########')\n",
    "    print('##############################################################')\n",
    "    # correction for float point at 0\n",
    "    lead_pred_sic[lead_pred_sic<0] = 0\n",
    "    # extend the dimension of sea ice mask\n",
    "    sea_ice_mask_test = np.repeat(sea_ice_mask_barents[np.newaxis,:,:],test_year*48,0)\n",
    "    # correct the land cells in the prediction\n",
    "    for i in range(step_lead):\n",
    "        lead_pred_sic[:,i,:,:] = lead_pred_sic[:,i,:,:] * sea_ice_mask_test\n",
    "    print('#################################################################################')\n",
    "    print('############   evaluation and statistical matrix for the entire year ############')\n",
    "    print('#################################################################################')\n",
    "    RMSE_ConvLSTM_0, RMSE_ConvLSTM_0_std = RMSE(lead_pred_sic[:,0,:,:] * sic_max,\n",
    "                                                sic_exp_norm[-test_year*12*4:,:,:] * sic_max)\n",
    "    RMSE_ConvLSTM_1, RMSE_ConvLSTM_1_std = RMSE(lead_pred_sic[:-1,1,:,:] * sic_max,\n",
    "                                                sic_exp_norm[-test_year*12*4+1:,:,:] * sic_max)\n",
    "    RMSE_ConvLSTM_2, RMSE_ConvLSTM_2_std = RMSE(lead_pred_sic[:-2,2,:,:] * sic_max,\n",
    "                                                sic_exp_norm[-test_year*12*4+2:,:,:] * sic_max)\n",
    "    RMSE_ConvLSTM_3, RMSE_ConvLSTM_3_std = RMSE(lead_pred_sic[:-3,3,:,:] * sic_max,\n",
    "                                                sic_exp_norm[-test_year*12*4+3:,:,:] * sic_max)\n",
    "    RMSE_ConvLSTM_4, RMSE_ConvLSTM_4_std = RMSE(lead_pred_sic[:-4,4,:,:] * sic_max,\n",
    "                                                sic_exp_norm[-test_year*12*4+4:,:,:] * sic_max)\n",
    "    RMSE_ConvLSTM_5, RMSE_ConvLSTM_5_std = RMSE(lead_pred_sic[:-5,5,:,:] * sic_max,\n",
    "                                                sic_exp_norm[-test_year*12*4+5:,:,:] * sic_max)\n",
    "    RMSE_ConvLSTM_6, RMSE_ConvLSTM_6_std = RMSE(lead_pred_sic[:-6,6,:,:] * sic_max,\n",
    "                                                sic_exp_norm[-test_year*12*4+6:,:,:] * sic_max)\n",
    "    RMSE_ConvLSTM_7, RMSE_ConvLSTM_7_std = RMSE(lead_pred_sic[:-7,7,:,:] * sic_max,\n",
    "                                                sic_exp_norm[-test_year*12*4+7:,:,:] * sic_max)\n",
    "    RMSE_ConvLSTM_8, RMSE_ConvLSTM_8_std = RMSE(lead_pred_sic[:-8,8,:,:] * sic_max,\n",
    "                                                sic_exp_norm[-test_year*12*4+8:,:,:] * sic_max)\n",
    "    RMSE_ConvLSTM_9, RMSE_ConvLSTM_9_std = RMSE(lead_pred_sic[:-9,9,:,:] * sic_max,\n",
    "                                                sic_exp_norm[-test_year*12*4+9:,:,:] * sic_max)\n",
    "    RMSE_ConvLSTM_10, RMSE_ConvLSTM_10_std = RMSE(lead_pred_sic[:-10,10,:,:] * sic_max,\n",
    "                                                sic_exp_norm[-test_year*12*4+10:,:,:] * sic_max)\n",
    "    RMSE_ConvLSTM_11, RMSE_ConvLSTM_11_std = RMSE(lead_pred_sic[:-11,11,:,:] * sic_max,\n",
    "                                                sic_exp_norm[-test_year*12*4+11:,:,:] * sic_max)\n",
    "    RMSE_ConvLSTM_12, RMSE_ConvLSTM_12_std = RMSE(lead_pred_sic[:-12,12,:,:] * sic_max,\n",
    "                                                sic_exp_norm[-test_year*12*4+12:,:,:] * sic_max)\n",
    "    RMSE_ConvLSTM_13, RMSE_ConvLSTM_13_std = RMSE(lead_pred_sic[:-13,13,:,:] * sic_max,\n",
    "                                                sic_exp_norm[-test_year*12*4+13:,:,:] * sic_max)\n",
    "    RMSE_ConvLSTM_14, RMSE_ConvLSTM_14_std = RMSE(lead_pred_sic[:-14,14,:,:] * sic_max,\n",
    "                                                sic_exp_norm[-test_year*12*4+14:,:,:] * sic_max)\n",
    "    RMSE_ConvLSTM_15, RMSE_ConvLSTM_15_std = RMSE(lead_pred_sic[:-15,15,:,:] * sic_max,\n",
    "                                                sic_exp_norm[-test_year*12*4+15:,:,:] * sic_max)\n",
    "    print(\"*******************     Lead time 0     *******************\")\n",
    "    print(\"Mean RMSE with testing data - ConvLSTM\")\n",
    "    print(np.mean(RMSE_ConvLSTM_0),\"+-\",np.mean(RMSE_ConvLSTM_0_std))\n",
    "    print(\"Mean RMSE with testing data - Climatology\")\n",
    "    print(RMSE_climatology,\"+-\",RMSE_climatology_std)\n",
    "    print(\"Mean RMSE with testing data - Persistence\")\n",
    "    print(np.mean(RMSE_persist_0),\"+-\",np.mean(RMSE_persist_0_std))\n",
    "    print(\"*******************     Lead time 1     *******************\")\n",
    "    print(\"Mean RMSE with testing data - ConvLSTM\")\n",
    "    print(np.mean(RMSE_ConvLSTM_1),\"+-\",np.mean(RMSE_ConvLSTM_1_std))\n",
    "    print(\"Mean RMSE with testing data - Persistence\")\n",
    "    print(np.mean(RMSE_persist_1),\"+-\",np.mean(RMSE_persist_1_std))\n",
    "    print(\"*******************     Lead time 2     *******************\")\n",
    "    print(\"Mean RMSE with testing data - ConvLSTM\")\n",
    "    print(np.mean(RMSE_ConvLSTM_2),\"+-\",np.mean(RMSE_ConvLSTM_2_std))\n",
    "    print(\"Mean RMSE with testing data - Persistence\")\n",
    "    print(np.mean(RMSE_persist_2),\"+-\",np.mean(RMSE_persist_2_std))\n",
    "    print(\"*******************     Lead time 3     *******************\")\n",
    "    print(\"Mean RMSE with testing data - ConvLSTM\")\n",
    "    print(np.mean(RMSE_ConvLSTM_3),\"+-\",np.mean(RMSE_ConvLSTM_3_std))\n",
    "    print(\"Mean RMSE with testing data - Persistence\")\n",
    "    print(np.mean(RMSE_persist_3),\"+-\",np.mean(RMSE_persist_3_std))\n",
    "    print(\"*******************     Lead time 4     *******************\")\n",
    "    print(\"Mean RMSE with testing data - ConvLSTM\")\n",
    "    print(np.mean(RMSE_ConvLSTM_4),\"+-\",np.mean(RMSE_ConvLSTM_4_std))\n",
    "    print(\"Mean RMSE with testing data - Persistence\")\n",
    "    print(np.mean(RMSE_persist_4),\"+-\",np.mean(RMSE_persist_4_std))\n",
    "    print(\"*******************     Lead time 5     *******************\")\n",
    "    print(\"Mean RMSE with testing data - ConvLSTM\")\n",
    "    print(np.mean(RMSE_ConvLSTM_5),\"+-\",np.mean(RMSE_ConvLSTM_5_std))\n",
    "    print(\"Mean RMSE with testing data - Persistence\")\n",
    "    print(np.mean(RMSE_persist_5),\"+-\",np.mean(RMSE_persist_5_std))\n",
    "    print(\"*******************     Lead time 6     *******************\")\n",
    "    print(\"Mean RMSE with testing data - ConvLSTM\")\n",
    "    print(np.mean(RMSE_ConvLSTM_6),\"+-\",np.mean(RMSE_ConvLSTM_6_std))\n",
    "    print(\"Mean RMSE with testing data - Persistence\")\n",
    "    print(np.mean(RMSE_persist_6),\"+-\",np.mean(RMSE_persist_6_std))\n",
    "    print(\"*******************     Lead time 7     *******************\")\n",
    "    print(\"Mean RMSE with testing data - ConvLSTM\")\n",
    "    print(np.mean(RMSE_ConvLSTM_7),\"+-\",np.mean(RMSE_ConvLSTM_7_std))\n",
    "    print(\"Mean RMSE with testing data - Persistence\")\n",
    "    print(np.mean(RMSE_persist_7),\"+-\",np.mean(RMSE_persist_7_std))\n",
    "    print(\"*******************     Lead time 8     *******************\")\n",
    "    print(\"Mean RMSE with testing data - ConvLSTM\")\n",
    "    print(np.mean(RMSE_ConvLSTM_8),\"+-\",np.mean(RMSE_ConvLSTM_8_std))\n",
    "    print(\"Mean RMSE with testing data - Persistence\")\n",
    "    print(np.mean(RMSE_persist_8),\"+-\",np.mean(RMSE_persist_8_std))\n",
    "    print(\"*******************     Lead time 9     *******************\")\n",
    "    print(\"Mean RMSE with testing data - ConvLSTM\")\n",
    "    print(np.mean(RMSE_ConvLSTM_9),\"+-\",np.mean(RMSE_ConvLSTM_9_std))\n",
    "    print(\"Mean RMSE with testing data - Persistence\")\n",
    "    print(np.mean(RMSE_persist_9),\"+-\",np.mean(RMSE_persist_9_std))\n",
    "    print(\"*******************     Lead time 10     *******************\")\n",
    "    print(\"Mean RMSE with testing data - ConvLSTM\")\n",
    "    print(np.mean(RMSE_ConvLSTM_10),\"+-\",np.mean(RMSE_ConvLSTM_10_std))\n",
    "    print(\"Mean RMSE with testing data - Persistence\")\n",
    "    print(np.mean(RMSE_persist_10),\"+-\",np.mean(RMSE_persist_10_std))\n",
    "    print(\"*******************     Lead time 11     *******************\")\n",
    "    print(\"Mean RMSE with testing data - ConvLSTM\")\n",
    "    print(np.mean(RMSE_ConvLSTM_11),\"+-\",np.mean(RMSE_ConvLSTM_11_std))\n",
    "    print(\"Mean RMSE with testing data - Persistence\")\n",
    "    print(np.mean(RMSE_persist_11),\"+-\",np.mean(RMSE_persist_11_std))\n",
    "    print(\"*******************     Lead time 12     *******************\")\n",
    "    print(\"Mean RMSE with testing data - ConvLSTM\")\n",
    "    print(np.mean(RMSE_ConvLSTM_12),\"+-\",np.mean(RMSE_ConvLSTM_12_std))\n",
    "    print(\"Mean RMSE with testing data - Persistence\")\n",
    "    print(np.mean(RMSE_persist_12),\"+-\",np.mean(RMSE_persist_12_std))\n",
    "    print(\"*******************     Lead time 13     *******************\")\n",
    "    print(\"Mean RMSE with testing data - ConvLSTM\")\n",
    "    print(np.mean(RMSE_ConvLSTM_13),\"+-\",np.mean(RMSE_ConvLSTM_13_std))\n",
    "    print(\"Mean RMSE with testing data - Persistence\")\n",
    "    print(np.mean(RMSE_persist_13),\"+-\",np.mean(RMSE_persist_13_std))\n",
    "    print(\"*******************     Lead time 14     *******************\")\n",
    "    print(\"Mean RMSE with testing data - ConvLSTM\")\n",
    "    print(np.mean(RMSE_ConvLSTM_14),\"+-\",np.mean(RMSE_ConvLSTM_14_std))\n",
    "    print(\"Mean RMSE with testing data - Persistence\")\n",
    "    print(np.mean(RMSE_persist_14),\"+-\",np.mean(RMSE_persist_14_std))\n",
    "    print(\"*******************     Lead time 15     *******************\")\n",
    "    print(\"Mean RMSE with testing data - ConvLSTM\")\n",
    "    print(np.mean(RMSE_ConvLSTM_15),\"+-\",np.mean(RMSE_ConvLSTM_15_std))\n",
    "    print(\"Mean RMSE with testing data - Persistence\")\n",
    "    print(np.mean(RMSE_persist_15),\"+-\",np.mean(RMSE_persist_15_std))\n",
    "    print('##############################################################')\n",
    "    print('############           create a txt file          ############')\n",
    "    print('##############################################################')\n",
    "    f = open(os.path.join(output_path,\"report_rmse_pred_16weeks.txt\"),\"w+\")\n",
    "    f.write(\"############   evaluation of prediction   ############\\n\")\n",
    "    f.write(\"############   Total RMSE with testing data   ############\\n\")\n",
    "    f.write(\"*******************     Lead time 0     *******************\\n\")\n",
    "    f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(RMSE_ConvLSTM_0),np.mean(RMSE_ConvLSTM_0_std)))\n",
    "    f.write(\"RMSE - Climatology    {} + - {}\\n\".format(RMSE_climatology, RMSE_climatology_std))\n",
    "    f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(RMSE_persist_0),np.mean(RMSE_persist_0_std)))\n",
    "    f.write(\"*******************     Lead time 1     *******************\\n\")\n",
    "    f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(RMSE_ConvLSTM_1),np.mean(RMSE_ConvLSTM_1_std)))\n",
    "    f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(RMSE_persist_1),np.mean(RMSE_persist_1_std)))\n",
    "    f.write(\"*******************     Lead time 2     *******************\\n\")\n",
    "    f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(RMSE_ConvLSTM_2),np.mean(RMSE_ConvLSTM_2_std)))\n",
    "    f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(RMSE_persist_2),np.mean(RMSE_persist_2_std)))\n",
    "    f.write(\"*******************     Lead time 3     *******************\\n\")\n",
    "    f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(RMSE_ConvLSTM_3),np.mean(RMSE_ConvLSTM_3_std)))\n",
    "    f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(RMSE_persist_3),np.mean(RMSE_persist_3_std)))\n",
    "    f.write(\"*******************     Lead time 4     *******************\\n\")\n",
    "    f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(RMSE_ConvLSTM_4),np.mean(RMSE_ConvLSTM_4_std)))\n",
    "    f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(RMSE_persist_4),np.mean(RMSE_persist_4_std)))\n",
    "    f.write(\"*******************     Lead time 5     *******************\\n\")\n",
    "    f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(RMSE_ConvLSTM_5),np.mean(RMSE_ConvLSTM_5_std)))\n",
    "    f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(RMSE_persist_5),np.mean(RMSE_persist_5_std)))\n",
    "    f.write(\"*******************     Lead time 6     *******************\\n\")\n",
    "    f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(RMSE_ConvLSTM_6),np.mean(RMSE_ConvLSTM_6_std)))\n",
    "    f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(RMSE_persist_6),np.mean(RMSE_persist_6_std)))\n",
    "    f.write(\"*******************     Lead time 7     *******************\\n\")\n",
    "    f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(RMSE_ConvLSTM_7),np.mean(RMSE_ConvLSTM_7_std)))\n",
    "    f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(RMSE_persist_7),np.mean(RMSE_persist_7_std)))\n",
    "    f.write(\"*******************     Lead time 8     *******************\\n\")\n",
    "    f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(RMSE_ConvLSTM_8),np.mean(RMSE_ConvLSTM_8_std)))\n",
    "    f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(RMSE_persist_8),np.mean(RMSE_persist_8_std)))\n",
    "    f.write(\"*******************     Lead time 9     *******************\\n\")\n",
    "    f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(RMSE_ConvLSTM_9),np.mean(RMSE_ConvLSTM_9_std)))\n",
    "    f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(RMSE_persist_9),np.mean(RMSE_persist_9_std)))\n",
    "    f.write(\"*******************     Lead time 10     *******************\\n\")\n",
    "    f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(RMSE_ConvLSTM_10),np.mean(RMSE_ConvLSTM_10_std)))\n",
    "    f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(RMSE_persist_10),np.mean(RMSE_persist_10_std)))\n",
    "    f.write(\"*******************     Lead time 11     *******************\\n\")\n",
    "    f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(RMSE_ConvLSTM_11),np.mean(RMSE_ConvLSTM_11_std)))\n",
    "    f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(RMSE_persist_11),np.mean(RMSE_persist_11_std)))\n",
    "    f.write(\"*******************     Lead time 12     *******************\\n\")\n",
    "    f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(RMSE_ConvLSTM_12),np.mean(RMSE_ConvLSTM_12_std)))\n",
    "    f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(RMSE_persist_12),np.mean(RMSE_persist_12_std)))\n",
    "    f.write(\"*******************     Lead time 13     *******************\\n\")\n",
    "    f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(RMSE_ConvLSTM_13),np.mean(RMSE_ConvLSTM_13_std)))\n",
    "    f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(RMSE_persist_13),np.mean(RMSE_persist_13_std)))\n",
    "    f.write(\"*******************     Lead time 14     *******************\\n\")\n",
    "    f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(RMSE_ConvLSTM_14),np.mean(RMSE_ConvLSTM_14_std)))\n",
    "    f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(RMSE_persist_14),np.mean(RMSE_persist_14_std)))\n",
    "    f.write(\"*******************     Lead time 15     *******************\\n\")\n",
    "    f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(RMSE_ConvLSTM_15),np.mean(RMSE_ConvLSTM_15_std)))\n",
    "    f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(RMSE_persist_15),np.mean(RMSE_persist_15_std)))\n",
    "    f.write(\"-\\n\")\n",
    "    f.write(\"-\\n\")\n",
    "    f.write(\"-\\n\")\n",
    "    f.write(\"############   Monthly RMSE with testing data   ############\\n\")\n",
    "    f.write(\"*******************     Lead time 0     *******************\\n\")\n",
    "    for i in np.arange(1,13,1):\n",
    "        ConvLSTM_monthly_series = week2month(lead_pred_sic[:,0,:,:], i)\n",
    "        persist_monthly_series = week2month(sic_exp_norm[-test_year*12*4-1:-1,:,:], i)\n",
    "        climatology_monthly_series = week2month(climatology_seq, i)\n",
    "        truth_monthly_series = week2month(sic_exp_norm[-test_year*12*4:,:,:], i)\n",
    "        \n",
    "        rmse_ConvLSTM_monthly, rmse_ConvLSTM_monthly_std = RMSE(ConvLSTM_monthly_series * sic_max,truth_monthly_series * sic_max)\n",
    "        rmse_persist_monthly, rmse_persist_monthly_std = RMSE(persist_monthly_series * sic_max,truth_monthly_series * sic_max)\n",
    "        rmse_climatology_monthly, rmse_climatology_monthly_std = RMSE(climatology_monthly_series * sic_max,truth_monthly_series * sic_max)\n",
    "        \n",
    "        print(\"*******************    {}     *******************\".format(i))\n",
    "        print(\"RMSE - ConvLSTM       {} + - {}\".format(np.mean(rmse_ConvLSTM_monthly), np.mean(rmse_ConvLSTM_monthly_std)))\n",
    "        print(\"RMSE - Climatology    {} + - {}\".format(np.mean(rmse_climatology_monthly), np.mean(rmse_climatology_monthly_std)))\n",
    "        print(\"RMSE - Persistence    {} + - {}\".format(np.mean(rmse_persist_monthly), np.mean(rmse_persist_monthly_std)))\n",
    "        f.write(\"*******************    {}     *******************\\n\".format(i))\n",
    "        f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(rmse_ConvLSTM_monthly), np.mean(rmse_ConvLSTM_monthly_std)))\n",
    "        f.write(\"RMSE - Climatology    {} + - {}\\n\".format(np.mean(rmse_climatology_monthly), np.mean(rmse_climatology_monthly_std)))\n",
    "        f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(rmse_persist_monthly), np.mean(rmse_persist_monthly_std)))\n",
    "        \n",
    "    #f.write(\"*******************     Lead time 6     *******************\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************  module for calculating accuracy/recall/precision  *********************\n"
     ]
    }
   ],
   "source": [
    "    print ('*******************  module for calculating accuracy/recall/precision  *********************')\n",
    "    def accuracy(pred, label):\n",
    "        #print(\"Input size must be [seq, lat, lon]\")\n",
    "        seq, lat, lon = pred.shape\n",
    "        boolean = (pred==label)\n",
    "        accu_seq = np.mean(np.mean(boolean.astype(float),2),1)\n",
    "        accu_spa = np.mean(boolean.astype(float),0)\n",
    "        \n",
    "        return accu_seq, accu_spa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #################################################################################\n",
    "    ########          transfer the sea ice fields into binary data           ########\n",
    "    #################################################################################\n",
    "    # ice concentration below the threshold is regarded as no ice, the value is from\n",
    "    # https://nsidc.org/cryosphere/seaice/data/terminology.html\n",
    "    criterion_0 = 0.15 \n",
    "    # remove the area weight\n",
    "    sic_exp_denorm = np.zeros(sic_exp_norm.shape, dtype=float)\n",
    "    lead_pred_sic_denorm = np.zeros(lead_pred_sic.shape, dtype=float)\n",
    "    for i in np.arange(height):\n",
    "        lead_pred_sic_denorm[:,:,i,:] = lead_pred_sic[:,:,i,:] /dx[i+12] * dx[35]\n",
    "        sic_exp_denorm[:,i,:] = sic_exp_norm[:,i,:] / dx[i+12] * dx[35]\n",
    "    # turn sea ice fields into binary data\n",
    "    lead_pred_sic_bin = lead_pred_sic_denorm[:]\n",
    "    sic_exp_bin = sic_exp_denorm[:]\n",
    "    lead_pred_sic_bin[lead_pred_sic_bin <= criterion_0] = 0\n",
    "    lead_pred_sic_bin[lead_pred_sic_bin > criterion_0] = 1\n",
    "    sic_exp_bin[sic_exp_bin <= criterion_0] = 0\n",
    "    sic_exp_bin[sic_exp_bin > criterion_0] = 1\n",
    "    # turn matrix into int\n",
    "    lead_pred_sic_bin = lead_pred_sic_bin.astype(int)\n",
    "    sic_exp_bin = sic_exp_bin.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################################################\n",
      "############   start prediction with climatology  ############\n",
      "##############################################################\n",
      "##############################################################\n",
      "############   start prediction with persistence  ############\n",
      "##############################################################\n",
      "##############################################################\n",
      "########   start prediction with linear regression   #########\n",
      "##############################################################\n",
      "#################################################################################\n",
      "############   evaluation and statistical matrix for the entire year ############\n",
      "#################################################################################\n",
      "*******************     Lead time 0     *******************\n",
      "Total accuracy with testing data - ConvLSTM\n",
      "0.9742024739583334\n",
      "Total accuracy with testing data - Climatology\n",
      "0.8557477678571428\n",
      "Total accuracy with testing data - Persistence\n",
      "0.9749193948412698\n",
      "*******************     Lead time 1     *******************\n",
      "Total accuracy with testing data - ConvLSTM\n",
      "0.9553493517825979\n",
      "Total accuracy with testing data - Persistence\n",
      "0.9597396222887059\n",
      "*******************     Lead time 2     *******************\n",
      "Total accuracy with testing data - ConvLSTM\n",
      "0.9427279135338344\n",
      "Total accuracy with testing data - Persistence\n",
      "0.9483787593984963\n",
      "*******************     Lead time 3     *******************\n",
      "Total accuracy with testing data - ConvLSTM\n",
      "0.9320475875535399\n",
      "Total accuracy with testing data - Persistence\n",
      "0.936889802217183\n",
      "*******************     Lead time 4     *******************\n",
      "Total accuracy with testing data - ConvLSTM\n",
      "0.9215583839918947\n",
      "Total accuracy with testing data - Persistence\n",
      "0.9253340298885512\n",
      "*******************     Lead time 5     *******************\n",
      "Total accuracy with testing data - ConvLSTM\n",
      "0.9106744970715559\n",
      "Total accuracy with testing data - Persistence\n",
      "0.9133960402342755\n",
      "##############################################################\n",
      "############           create a txt file          ############\n",
      "##############################################################\n",
      "*******************    1     *******************\n",
      "accuracy - ConvLSTM       0.9496372767857143\n",
      "accuracy - Climatology    0.8189174107142857\n",
      "accuracy - Persistence    0.9498697916666667\n",
      "*******************    2     *******************\n",
      "accuracy - ConvLSTM       0.9633556547619048\n",
      "accuracy - Climatology    0.8280784970238095\n",
      "accuracy - Persistence    0.9636811755952381\n",
      "*******************    3     *******************\n",
      "accuracy - ConvLSTM       0.9649367559523809\n",
      "accuracy - Climatology    0.8337983630952381\n",
      "accuracy - Persistence    0.9605189732142858\n",
      "*******************    4     *******************\n",
      "accuracy - ConvLSTM       0.9706566220238095\n",
      "accuracy - Climatology    0.8166852678571428\n",
      "accuracy - Persistence    0.968563988095238\n",
      "*******************    5     *******************\n",
      "accuracy - ConvLSTM       0.9681454613095238\n",
      "accuracy - Climatology    0.7997116815476191\n",
      "accuracy - Persistence    0.9637276785714285\n",
      "*******************    6     *******************\n",
      "accuracy - ConvLSTM       0.9808872767857144\n",
      "accuracy - Climatology    0.8109654017857143\n",
      "accuracy - Persistence    0.9734933035714286\n",
      "*******************    7     *******************\n",
      "accuracy - ConvLSTM       0.9902808779761905\n",
      "accuracy - Climatology    0.905087425595238\n",
      "accuracy - Persistence    0.9868396577380952\n",
      "*******************    8     *******************\n",
      "accuracy - ConvLSTM       0.9959077380952381\n",
      "accuracy - Climatology    0.9574032738095237\n",
      "accuracy - Persistence    0.9961402529761905\n",
      "*******************    9     *******************\n",
      "accuracy - ConvLSTM       0.9962332589285714\n",
      "accuracy - Climatology    0.9603329613095238\n",
      "accuracy - Persistence    0.998046875\n",
      "*******************    10     *******************\n",
      "accuracy - ConvLSTM       0.9816313244047619\n",
      "accuracy - Climatology    0.9025762648809523\n",
      "accuracy - Persistence    0.9930245535714286\n",
      "*******************    11     *******************\n",
      "accuracy - ConvLSTM       0.9683314732142857\n",
      "accuracy - Climatology    0.8140811011904763\n",
      "accuracy - Persistence    0.9844680059523809\n",
      "*******************    12     *******************\n",
      "accuracy - ConvLSTM       0.9604259672619049\n",
      "accuracy - Climatology    0.8213355654761905\n",
      "accuracy - Persistence    0.9606584821428572\n"
     ]
    }
   ],
   "source": [
    "    #################################################################################\n",
    "    ########              performance evaluation with accuracy               ########\n",
    "    #################################################################################\n",
    "    sequence_len, height, width = sic_exp_norm.shape\n",
    "    print('##############################################################')\n",
    "    print('############   start prediction with climatology  ############')\n",
    "    print('##############################################################')\n",
    "    # compute climatology\n",
    "    climatology = np.zeros((48, height, width),dtype=float)\n",
    "    for i in range(48):\n",
    "        climatology[i,:,:] = np.mean(sic_exp_norm[i::48,:,:],axis=0)\n",
    "    # repeat this climatology and calculate the RMSE\n",
    "    climatology_seq = np.tile(climatology,(test_year,1,1))\n",
    "    # turn into binary\n",
    "    climatology_denorm = np.zeros(climatology_seq.shape, dtype=float)\n",
    "    for i in np.arange(height):\n",
    "        climatology_denorm[:,i,:] = climatology_seq[:,i,:] / dx[i+12] * dx[35]    \n",
    "    climatology_denorm[climatology_denorm <= criterion_0] = 0\n",
    "    climatology_denorm[climatology_denorm > criterion_0] = 1\n",
    "    # turn matrix into int\n",
    "    climatology_denorm = climatology_denorm.astype(int)\n",
    "    \n",
    "    accu_climatology, _ = accuracy(climatology_denorm, sic_exp_bin[-test_year*12*4:,:,:])\n",
    "    print('##############################################################')\n",
    "    print('############   start prediction with persistence  ############')\n",
    "    print('##############################################################')\n",
    "    accu_persist_0, _ = accuracy(sic_exp_bin[-test_year*12*4-1:-1,:,:],sic_exp_bin[-test_year*12*4:,:,:])\n",
    "    accu_persist_1, _ = accuracy(sic_exp_bin[-test_year*12*4-1:-2,:,:],sic_exp_bin[-test_year*12*4+1:,:,:])\n",
    "    accu_persist_2, _ = accuracy(sic_exp_bin[-test_year*12*4-1:-3,:,:],sic_exp_bin[-test_year*12*4+2:,:,:])\n",
    "    accu_persist_3, _ = accuracy(sic_exp_bin[-test_year*12*4-1:-4,:,:],sic_exp_bin[-test_year*12*4+3:,:,:])\n",
    "    accu_persist_4, _ = accuracy(sic_exp_bin[-test_year*12*4-1:-5,:,:],sic_exp_bin[-test_year*12*4+4:,:,:])\n",
    "    accu_persist_5, _ = accuracy(sic_exp_bin[-test_year*12*4-1:-6,:,:],sic_exp_bin[-test_year*12*4+5:,:,:])\n",
    "    print('##############################################################')\n",
    "    print('########   start prediction with linear regression   #########')\n",
    "    print('##############################################################')\n",
    "    \n",
    "    print('#################################################################################')\n",
    "    print('############   evaluation and statistical matrix for the entire year ############')\n",
    "    print('#################################################################################')\n",
    "    accu_ConvLSTM_0, _ = accuracy(lead_pred_sic_bin[:,0,:,:],sic_exp_bin[-test_year*12*4:,:,:])\n",
    "    accu_ConvLSTM_1, _ = accuracy(lead_pred_sic_bin[:-1,1,:,:],sic_exp_bin[-test_year*12*4+1:,:,:])\n",
    "    accu_ConvLSTM_2, _ = accuracy(lead_pred_sic_bin[:-2,2,:,:],sic_exp_bin[-test_year*12*4+2:,:,:])\n",
    "    accu_ConvLSTM_3, _ = accuracy(lead_pred_sic_bin[:-3,3,:,:],sic_exp_bin[-test_year*12*4+3:,:,:])\n",
    "    accu_ConvLSTM_4, _ = accuracy(lead_pred_sic_bin[:-4,4,:,:],sic_exp_bin[-test_year*12*4+4:,:,:])\n",
    "    accu_ConvLSTM_5, _ = accuracy(lead_pred_sic_bin[:-5,5,:,:],sic_exp_bin[-test_year*12*4+5:,:,:])\n",
    "    print(\"*******************     Lead time 0     *******************\")\n",
    "    print(\"Total accuracy with testing data - ConvLSTM\")\n",
    "    print(np.mean(accu_ConvLSTM_0))\n",
    "    print(\"Total accuracy with testing data - Climatology\")\n",
    "    print(np.mean(accu_climatology))\n",
    "    print(\"Total accuracy with testing data - Persistence\")\n",
    "    print(np.mean(accu_persist_0))\n",
    "    print(\"*******************     Lead time 1     *******************\")\n",
    "    print(\"Total accuracy with testing data - ConvLSTM\")\n",
    "    print(np.mean(accu_ConvLSTM_1))\n",
    "    print(\"Total accuracy with testing data - Persistence\")\n",
    "    print(np.mean(accu_persist_1))\n",
    "    print(\"*******************     Lead time 2     *******************\")\n",
    "    print(\"Total accuracy with testing data - ConvLSTM\")\n",
    "    print(np.mean(accu_ConvLSTM_2))\n",
    "    print(\"Total accuracy with testing data - Persistence\")\n",
    "    print(np.mean(accu_persist_2))\n",
    "    print(\"*******************     Lead time 3     *******************\")\n",
    "    print(\"Total accuracy with testing data - ConvLSTM\")\n",
    "    print(np.mean(accu_ConvLSTM_3))\n",
    "    print(\"Total accuracy with testing data - Persistence\")\n",
    "    print(np.mean(accu_persist_3))\n",
    "    print(\"*******************     Lead time 4     *******************\")\n",
    "    print(\"Total accuracy with testing data - ConvLSTM\")\n",
    "    print(np.mean(accu_ConvLSTM_4))\n",
    "    print(\"Total accuracy with testing data - Persistence\")\n",
    "    print(np.mean(accu_persist_4))\n",
    "    print(\"*******************     Lead time 5     *******************\")\n",
    "    print(\"Total accuracy with testing data - ConvLSTM\")\n",
    "    print(np.mean(accu_ConvLSTM_5))\n",
    "    print(\"Total accuracy with testing data - Persistence\")\n",
    "    print(np.mean(accu_persist_5))\n",
    "    print('##############################################################')\n",
    "    print('############           create a txt file          ############')\n",
    "    print('##############################################################')\n",
    "    f = open(os.path.join(output_path,\"report_accuracy_pred_bin.txt\"),\"w+\")\n",
    "    f.write(\"############   evaluation of prediction   ############\\n\")\n",
    "    f.write(\"############   Total accuracy with testing data   ############\\n\")\n",
    "    f.write(\"*******************     Lead time 0     *******************\\n\")\n",
    "    f.write(\"accuracy - ConvLSTM       {}\\n\".format(np.mean(accu_ConvLSTM_0)))\n",
    "    f.write(\"accuracy - Climatology    {}\\n\".format(np.mean(accu_climatology)))\n",
    "    f.write(\"accuracy - Persistence    {}\\n\".format(np.mean(accu_persist_0)))\n",
    "    f.write(\"*******************     Lead time 1     *******************\\n\")\n",
    "    f.write(\"accuracy - ConvLSTM       {}\\n\".format(np.mean(accu_ConvLSTM_1)))\n",
    "    f.write(\"accuracy - Persistence    {}\\n\".format(np.mean(accu_persist_1)))\n",
    "    f.write(\"*******************     Lead time 2     *******************\\n\")\n",
    "    f.write(\"accuracy - ConvLSTM       {}\\n\".format(np.mean(accu_ConvLSTM_2)))\n",
    "    f.write(\"accuracy - Persistence    {}\\n\".format(np.mean(accu_persist_2)))\n",
    "    f.write(\"*******************     Lead time 3     *******************\\n\")\n",
    "    f.write(\"accuracy - ConvLSTM       {}\\n\".format(np.mean(accu_ConvLSTM_3)))\n",
    "    f.write(\"accuracy - Persistence    {}\\n\".format(np.mean(accu_persist_3)))\n",
    "    f.write(\"*******************     Lead time 4     *******************\\n\")\n",
    "    f.write(\"accuracy - ConvLSTM       {}\\n\".format(np.mean(accu_ConvLSTM_4)))\n",
    "    f.write(\"accuracy - Persistence    {}\\n\".format(np.mean(accu_persist_4)))\n",
    "    f.write(\"*******************     Lead time 5     *******************\\n\")\n",
    "    f.write(\"accuracy - ConvLSTM       {}\\n\".format(np.mean(accu_ConvLSTM_5)))\n",
    "    f.write(\"accuracy - Persistence    {}\\n\".format(np.mean(accu_persist_5)))\n",
    "    f.write(\"-\\n\")\n",
    "    f.write(\"-\\n\")\n",
    "    f.write(\"-\\n\")\n",
    "    f.write(\"############   Monthly accuracy with testing data   ############\\n\")\n",
    "    f.write(\"*******************     Lead time 0     *******************\\n\")\n",
    "    for i in np.arange(1,13,1):\n",
    "        ConvLSTM_monthly_series = np.zeros((test_year*4,height, width),dtype=float)\n",
    "        persist_monthly_series = np.zeros((test_year*4,height, width),dtype=float)\n",
    "        climatology_monthly_series = np.zeros((test_year*4,height, width),dtype=float)\n",
    "        truth_monthly_series = np.zeros((test_year*4,height, width),dtype=float)\n",
    "        for j in np.arange(4):\n",
    "            ConvLSTM_monthly_series[j::4,:,:] = lead_pred_sic_bin[(i-1)*4+j::48,0,:,:]\n",
    "            persist_monthly_series[j::4,:,:] = sic_exp_bin[-test_year*12*4-1+(i-1)*4+j:-1:48,:,:]\n",
    "            climatology_monthly_series[j::4,:,:] = climatology_denorm[(i-1)*4+j::48,:,:]\n",
    "            truth_monthly_series[j::4,:,:] = sic_exp_bin[-test_year*12*4+(i-1)*4+j::48,:,:]\n",
    "        print(\"*******************    {}     *******************\".format(i))\n",
    "        print(\"accuracy - ConvLSTM       {}\".format(np.mean(accuracy(ConvLSTM_monthly_series,truth_monthly_series)[0])))\n",
    "        print(\"accuracy - Climatology    {}\".format(np.mean(accuracy(climatology_monthly_series,truth_monthly_series)[0])))\n",
    "        print(\"accuracy - Persistence    {}\".format(np.mean(accuracy(persist_monthly_series,truth_monthly_series)[0])))\n",
    "        f.write(\"*******************    {}     *******************\\n\".format(i))\n",
    "        f.write(\"accuracy - ConvLSTM       {}\\n\".format(np.mean(accuracy(ConvLSTM_monthly_series,truth_monthly_series)[0])))\n",
    "        f.write(\"accuracy - Climatology    {}\\n\".format(np.mean(accuracy(climatology_monthly_series,truth_monthly_series)[0])))\n",
    "        f.write(\"accuracy - Persistence    {}\\n\".format(np.mean(accuracy(persist_monthly_series,truth_monthly_series)[0])))\n",
    "    f.write(\"*******************     Lead time 6     *******************\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************  module for calculating recall/precision  *********************\n"
     ]
    }
   ],
   "source": [
    "    print ('*******************  module for calculating recall/precision  *********************')\n",
    "    # positive is sea ice = 1\n",
    "    \n",
    "    def recall(pred, label):\n",
    "        \"\"\"\n",
    "        True positive / Total actual positive\n",
    "        Input fields must contain only 0 / 1. 1 is positive.\n",
    "        \"\"\"\n",
    "        #print(\"Input size must be [seq, lat, lon]\")\n",
    "        seq, lat, lon = pred.shape\n",
    "        # initialize dummy matrix\n",
    "        pred_dummy_1 = np.zeros(pred.shape,dtype=int)\n",
    "        label_dummy_1 = np.zeros(label.shape,dtype=int)\n",
    "        # True positive\n",
    "        # create dummy matrix to save the labels\n",
    "        pred_dummy_1[:] = pred[:]\n",
    "        label_dummy_1[:] = label[:]\n",
    "        # change the label of negative events\n",
    "        pred_dummy_1[pred == 0] = 2\n",
    "        label_dummy_1[label == 0] = 3\n",
    "        # count True Positive events\n",
    "        truePositive = (pred_dummy_1 == label_dummy_1)\n",
    "\n",
    "        # initialize dummy matrix\n",
    "        pred_dummy_2 = np.zeros(pred.shape,dtype=int)\n",
    "        label_dummy_2 = np.zeros(label.shape,dtype=int)\n",
    "        # False negative (is 1 but predict 0)\n",
    "        # create dummy matrix to save the labels (reset dummy)\n",
    "        pred_dummy_2[:] = pred[:]\n",
    "        label_dummy_2[:] = label[:]\n",
    "        pred_dummy_2[pred == 0] = 2\n",
    "        label_dummy_2[label == 1] = 2\n",
    "        # count False Positive events\n",
    "        falseNegative = (pred_dummy_2 == label_dummy_2)\n",
    "\n",
    "#         recall_seq = np.mean(np.mean(np.nan_to_num(truePositive.astype(float) / \n",
    "#                                     (truePositive.astype(float) + falseNegative.astype(float))),2),1)\n",
    "        \n",
    "        recall_seq = np.sum(np.sum(truePositive.astype(float),2),1) / (np.sum(np.sum(truePositive.astype(float),2),1) +\n",
    "                                                                       np.sum(np.sum(falseNegative.astype(float),2),1))\n",
    "        \n",
    "#         recall_spa = np.mean(np.nan_to_num(truePositive.astype(float) / \n",
    "#                                           (truePositive.astype(float) + falseNegative.astype(float))),0)\n",
    "        \n",
    "        recall_spa = np.sum(truePositive.astype(float),0) / (np.sum(truePositive.astype(float),0) +\n",
    "                                                             np.sum(falseNegative.astype(float),0))\n",
    "        \n",
    "        #return recall_seq, recall_spa\n",
    "        return np.nan_to_num(recall_seq), np.nan_to_num(recall_spa)\n",
    "    \n",
    "    def precision(pred, label):\n",
    "        \"\"\"\n",
    "        True positive / Total predicted positive\n",
    "        Input fields must contain only 0 / 1. 1 is positive.\n",
    "        \"\"\"\n",
    "        #print(\"Input size must be [seq, lat, lon]\")\n",
    "        seq, lat, lon = pred.shape\n",
    "        # initialize dummy matrix\n",
    "        pred_dummy_1 = np.zeros(pred.shape,dtype=int)\n",
    "        label_dummy_1 = np.zeros(label.shape,dtype=int)\n",
    "        # True positive\n",
    "        # create dummy matrix to save the labels\n",
    "        pred_dummy_1[:] = pred[:]\n",
    "        label_dummy_1[:] = label[:]\n",
    "        # change the label of negative events\n",
    "        pred_dummy_1[pred == 0] = 2\n",
    "        label_dummy_1[label == 0] = 3\n",
    "        # count True Positive events\n",
    "        truePositive = (pred_dummy_1 == label_dummy_1)\n",
    "\n",
    "        # initialize dummy matrix\n",
    "        pred_dummy_2 = np.zeros(pred.shape,dtype=int)\n",
    "        label_dummy_2 = np.zeros(label.shape,dtype=int)\n",
    "        # False positive (is 0 but predict 1)\n",
    "        # create dummy matrix to save the labels (reset dummy)\n",
    "        pred_dummy_2[:] = pred[:]\n",
    "        label_dummy_2[:] = label[:]\n",
    "        pred_dummy_2[pred == 1] = 2\n",
    "        label_dummy_2[label == 0] = 2\n",
    "        # count False Positive events\n",
    "        falsePositive = (pred_dummy_2 == label_dummy_2)\n",
    "        \n",
    "        prec_seq = np.sum(np.sum(truePositive.astype(float),2),1) / (np.sum(np.sum(truePositive.astype(float),2),1) +\n",
    "                                                                     np.sum(np.sum(falsePositive.astype(float),2),1))\n",
    "        \n",
    "        prec_spa = np.sum(truePositive.astype(float),0) / (np.sum(truePositive.astype(float),0) +\n",
    "                                                           np.sum(falsePositive.astype(float),0))\n",
    "        \n",
    "        return np.nan_to_num(prec_seq), np.nan_to_num(prec_spa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################################################\n",
      "############   start prediction with climatology  ############\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:40: RuntimeWarning: invalid value encountered in true_divide\n",
      "G:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:46: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################################################\n",
      "############   start prediction with persistence  ############\n",
      "##############################################################\n",
      "##############################################################\n",
      "########   start prediction with linear regression   #########\n",
      "##############################################################\n",
      "#################################################################################\n",
      "############   evaluation and statistical matrix for the entire year ############\n",
      "#################################################################################\n",
      "*******************     Lead time 0     *******************\n",
      "Total recall with testing data - ConvLSTM\n",
      "0.7294511802947931\n",
      "Total recall with testing data - Climatology\n",
      "0.7682106239728338\n",
      "Total recall with testing data - Persistence\n",
      "0.6956084994582069\n",
      "*******************     Lead time 1     *******************\n",
      "Total recall with testing data - ConvLSTM\n",
      "0.7050962750794713\n",
      "Total recall with testing data - Persistence\n",
      "0.6436574679557777\n",
      "*******************     Lead time 2     *******************\n",
      "Total recall with testing data - ConvLSTM\n",
      "0.6909375651664893\n",
      "Total recall with testing data - Persistence\n",
      "0.610596193945815\n",
      "*******************     Lead time 3     *******************\n",
      "Total recall with testing data - ConvLSTM\n",
      "0.6749170520140996\n",
      "Total recall with testing data - Persistence\n",
      "0.5745854442827217\n",
      "*******************     Lead time 4     *******************\n",
      "Total recall with testing data - ConvLSTM\n",
      "0.6594568455857532\n",
      "Total recall with testing data - Persistence\n",
      "0.5492875256810292\n",
      "*******************     Lead time 5     *******************\n",
      "Total recall with testing data - ConvLSTM\n",
      "0.6446762471590884\n",
      "Total recall with testing data - Persistence\n",
      "0.5259148468998884\n",
      "##############################################################\n",
      "############           create a txt file          ############\n",
      "##############################################################\n",
      "*******************    1     *******************\n",
      "recall - ConvLSTM       0.860598886960916\n",
      "recall - Climatology    0.9985675754922358\n",
      "recall - Persistence    0.8314841347620789\n",
      "*******************    2     *******************\n",
      "recall - ConvLSTM       0.9453464077344558\n",
      "recall - Climatology    0.9988817097415507\n",
      "recall - Persistence    0.9345966147874042\n",
      "*******************    3     *******************\n",
      "recall - ConvLSTM       0.9346326494497246\n",
      "recall - Climatology    0.9997340425531915\n",
      "recall - Persistence    0.9106219389571815\n",
      "*******************    4     *******************\n",
      "recall - ConvLSTM       0.9685899947294929\n",
      "recall - Climatology    0.998480953251909\n",
      "recall - Persistence    0.967307576852602\n",
      "*******************    5     *******************\n",
      "recall - ConvLSTM       0.9754766488516187\n",
      "recall - Climatology    0.9983039583340307\n",
      "recall - Persistence    0.980571197002486\n",
      "*******************    6     *******************\n",
      "recall - ConvLSTM       0.9473765015441237\n",
      "recall - Climatology    0.99876677278424\n",
      "recall - Persistence    0.97613758406689\n",
      "*******************    7     *******************\n",
      "recall - ConvLSTM       0.6955869319793339\n",
      "recall - Climatology    0.6825157351595051\n",
      "recall - Persistence    0.731919166058951\n",
      "*******************    8     *******************\n",
      "recall - ConvLSTM       0.25801363975690794\n",
      "recall - Climatology    0.17666547941988756\n",
      "recall - Persistence    0.2848559532321151\n",
      "*******************    9     *******************\n",
      "recall - ConvLSTM       0.24191735537190082\n",
      "recall - Climatology    0.08532851239669421\n",
      "recall - Persistence    0.24395041322314048\n",
      "*******************    10     *******************\n",
      "recall - ConvLSTM       0.3462687717754512\n",
      "recall - Climatology    0.31790314876598696\n",
      "recall - Persistence    0.2090523667871815\n",
      "*******************    11     *******************\n",
      "recall - ConvLSTM       0.7261897291259987\n",
      "recall - Climatology    0.96659537082369\n",
      "recall - Persistence    0.5508195922988457\n",
      "*******************    12     *******************\n",
      "recall - ConvLSTM       0.8534166462575934\n",
      "recall - Climatology    0.9967842289510855\n",
      "recall - Persistence    0.7259854554696066\n"
     ]
    }
   ],
   "source": [
    "    #################################################################################\n",
    "    ########                performance evaluation with recall               ########\n",
    "    #################################################################################\n",
    "    sequence_len, height, width = sic_exp_norm.shape\n",
    "    print('##############################################################')\n",
    "    print('############   start prediction with climatology  ############')\n",
    "    print('##############################################################')\n",
    "    # compute climatology\n",
    "    climatology = np.zeros((48, height, width),dtype=float)\n",
    "    for i in range(48):\n",
    "        climatology[i,:,:] = np.mean(sic_exp_norm[i::48,:,:],axis=0)\n",
    "    # repeat this climatology and calculate the RMSE\n",
    "    climatology_seq = np.tile(climatology,(test_year,1,1))\n",
    "    # turn into binary\n",
    "    climatology_denorm = np.zeros(climatology_seq.shape, dtype=float)\n",
    "    for i in np.arange(height):\n",
    "        climatology_denorm[:,i,:] = climatology_seq[:,i,:] / dx[i+12] * dx[35]    \n",
    "    climatology_denorm[climatology_denorm <= criterion_0] = 0\n",
    "    climatology_denorm[climatology_denorm > criterion_0] = 1\n",
    "    # turn matrix into int\n",
    "    climatology_denorm = climatology_denorm.astype(int)\n",
    "    \n",
    "    recall_climatology, _ = recall(climatology_denorm, sic_exp_bin[-test_year*12*4:,:,:])\n",
    "    print('##############################################################')\n",
    "    print('############   start prediction with persistence  ############')\n",
    "    print('##############################################################')\n",
    "    recall_persist_0, _ = recall(sic_exp_bin[-test_year*12*4-1:-1,:,:],sic_exp_bin[-test_year*12*4:,:,:])\n",
    "    recall_persist_1, _ = recall(sic_exp_bin[-test_year*12*4-1:-2,:,:],sic_exp_bin[-test_year*12*4+1:,:,:])\n",
    "    recall_persist_2, _ = recall(sic_exp_bin[-test_year*12*4-1:-3,:,:],sic_exp_bin[-test_year*12*4+2:,:,:])\n",
    "    recall_persist_3, _ = recall(sic_exp_bin[-test_year*12*4-1:-4,:,:],sic_exp_bin[-test_year*12*4+3:,:,:])\n",
    "    recall_persist_4, _ = recall(sic_exp_bin[-test_year*12*4-1:-5,:,:],sic_exp_bin[-test_year*12*4+4:,:,:])\n",
    "    recall_persist_5, _ = recall(sic_exp_bin[-test_year*12*4-1:-6,:,:],sic_exp_bin[-test_year*12*4+5:,:,:])\n",
    "    print('##############################################################')\n",
    "    print('########   start prediction with linear regression   #########')\n",
    "    print('##############################################################')\n",
    "    \n",
    "    print('#################################################################################')\n",
    "    print('############   evaluation and statistical matrix for the entire year ############')\n",
    "    print('#################################################################################')\n",
    "    recall_ConvLSTM_0, _ = recall(lead_pred_sic_bin[:,0,:,:],sic_exp_bin[-test_year*12*4:,:,:])\n",
    "    recall_ConvLSTM_1, _ = recall(lead_pred_sic_bin[:-1,1,:,:],sic_exp_bin[-test_year*12*4+1:,:,:])\n",
    "    recall_ConvLSTM_2, _ = recall(lead_pred_sic_bin[:-2,2,:,:],sic_exp_bin[-test_year*12*4+2:,:,:])\n",
    "    recall_ConvLSTM_3, _ = recall(lead_pred_sic_bin[:-3,3,:,:],sic_exp_bin[-test_year*12*4+3:,:,:])\n",
    "    recall_ConvLSTM_4, _ = recall(lead_pred_sic_bin[:-4,4,:,:],sic_exp_bin[-test_year*12*4+4:,:,:])\n",
    "    recall_ConvLSTM_5, _ = recall(lead_pred_sic_bin[:-5,5,:,:],sic_exp_bin[-test_year*12*4+5:,:,:])\n",
    "    print(\"*******************     Lead time 0     *******************\")\n",
    "    print(\"Total recall with testing data - ConvLSTM\")\n",
    "    print(np.mean(recall_ConvLSTM_0))\n",
    "    print(\"Total recall with testing data - Climatology\")\n",
    "    print(np.mean(recall_climatology))\n",
    "    print(\"Total recall with testing data - Persistence\")\n",
    "    print(np.mean(recall_persist_0))\n",
    "    print(\"*******************     Lead time 1     *******************\")\n",
    "    print(\"Total recall with testing data - ConvLSTM\")\n",
    "    print(np.mean(recall_ConvLSTM_1))\n",
    "    print(\"Total recall with testing data - Persistence\")\n",
    "    print(np.mean(recall_persist_1))\n",
    "    print(\"*******************     Lead time 2     *******************\")\n",
    "    print(\"Total recall with testing data - ConvLSTM\")\n",
    "    print(np.mean(recall_ConvLSTM_2))\n",
    "    print(\"Total recall with testing data - Persistence\")\n",
    "    print(np.mean(recall_persist_2))\n",
    "    print(\"*******************     Lead time 3     *******************\")\n",
    "    print(\"Total recall with testing data - ConvLSTM\")\n",
    "    print(np.mean(recall_ConvLSTM_3))\n",
    "    print(\"Total recall with testing data - Persistence\")\n",
    "    print(np.mean(recall_persist_3))\n",
    "    print(\"*******************     Lead time 4     *******************\")\n",
    "    print(\"Total recall with testing data - ConvLSTM\")\n",
    "    print(np.mean(recall_ConvLSTM_4))\n",
    "    print(\"Total recall with testing data - Persistence\")\n",
    "    print(np.mean(recall_persist_4))\n",
    "    print(\"*******************     Lead time 5     *******************\")\n",
    "    print(\"Total recall with testing data - ConvLSTM\")\n",
    "    print(np.mean(recall_ConvLSTM_5))\n",
    "    print(\"Total recall with testing data - Persistence\")\n",
    "    print(np.mean(recall_persist_5))\n",
    "    print('##############################################################')\n",
    "    print('############           create a txt file          ############')\n",
    "    print('##############################################################')\n",
    "    f = open(os.path.join(output_path,\"report_recall_pred_bin.txt\"),\"w+\")\n",
    "    f.write(\"############   evaluation of prediction   ############\\n\")\n",
    "    f.write(\"############   Total recall with testing data   ############\\n\")\n",
    "    f.write(\"*******************     Lead time 0     *******************\\n\")\n",
    "    f.write(\"recall - ConvLSTM       {}\\n\".format(np.mean(recall_ConvLSTM_0)))\n",
    "    f.write(\"recall - Climatology    {}\\n\".format(np.mean(recall_climatology)))\n",
    "    f.write(\"recall - Persistence    {}\\n\".format(np.mean(recall_persist_0)))\n",
    "    f.write(\"*******************     Lead time 1     *******************\\n\")\n",
    "    f.write(\"recall - ConvLSTM       {}\\n\".format(np.mean(recall_ConvLSTM_1)))\n",
    "    f.write(\"recall - Persistence    {}\\n\".format(np.mean(recall_persist_1)))\n",
    "    f.write(\"*******************     Lead time 2     *******************\\n\")\n",
    "    f.write(\"recall - ConvLSTM       {}\\n\".format(np.mean(recall_ConvLSTM_2)))\n",
    "    f.write(\"recall - Persistence    {}\\n\".format(np.mean(recall_persist_2)))\n",
    "    f.write(\"*******************     Lead time 3     *******************\\n\")\n",
    "    f.write(\"recall - ConvLSTM       {}\\n\".format(np.mean(recall_ConvLSTM_3)))\n",
    "    f.write(\"recall - Persistence    {}\\n\".format(np.mean(recall_persist_3)))\n",
    "    f.write(\"*******************     Lead time 4     *******************\\n\")\n",
    "    f.write(\"recall - ConvLSTM       {}\\n\".format(np.mean(recall_ConvLSTM_4)))\n",
    "    f.write(\"recall - Persistence    {}\\n\".format(np.mean(recall_persist_4)))\n",
    "    f.write(\"*******************     Lead time 5     *******************\\n\")\n",
    "    f.write(\"recall - ConvLSTM       {}\\n\".format(np.mean(recall_ConvLSTM_5)))\n",
    "    f.write(\"recall - Persistence    {}\\n\".format(np.mean(recall_persist_5)))\n",
    "    f.write(\"-\\n\")\n",
    "    f.write(\"-\\n\")\n",
    "    f.write(\"-\\n\")\n",
    "    f.write(\"############   Monthly accuracy with testing data   ############\\n\")\n",
    "    f.write(\"*******************     Lead time 0     *******************\\n\")\n",
    "    for i in np.arange(1,13,1):\n",
    "        ConvLSTM_monthly_series = np.zeros((test_year*4,height, width),dtype=float)\n",
    "        persist_monthly_series = np.zeros((test_year*4,height, width),dtype=float)\n",
    "        climatology_monthly_series = np.zeros((test_year*4,height, width),dtype=float)\n",
    "        truth_monthly_series = np.zeros((test_year*4,height, width),dtype=float)\n",
    "        for j in np.arange(4):\n",
    "            ConvLSTM_monthly_series[j::4,:,:] = lead_pred_sic_bin[(i-1)*4+j::48,0,:,:]\n",
    "            persist_monthly_series[j::4,:,:] = sic_exp_bin[-test_year*12*4-1+(i-1)*4+j:-1:48,:,:]\n",
    "            climatology_monthly_series[j::4,:,:] = climatology_denorm[(i-1)*4+j::48,:,:]\n",
    "            truth_monthly_series[j::4,:,:] = sic_exp_bin[-test_year*12*4+(i-1)*4+j::48,:,:]\n",
    "        print(\"*******************    {}     *******************\".format(i))\n",
    "        print(\"recall - ConvLSTM       {}\".format(np.mean(recall(ConvLSTM_monthly_series,truth_monthly_series)[0])))\n",
    "        print(\"recall - Climatology    {}\".format(np.mean(recall(climatology_monthly_series,truth_monthly_series)[0])))\n",
    "        print(\"recall - Persistence    {}\".format(np.mean(recall(persist_monthly_series,truth_monthly_series)[0])))\n",
    "        f.write(\"*******************    {}     *******************\\n\".format(i))\n",
    "        f.write(\"recall - ConvLSTM       {}\\n\".format(np.mean(recall(ConvLSTM_monthly_series,truth_monthly_series)[0])))\n",
    "        f.write(\"recall - Climatology    {}\\n\".format(np.mean(recall(climatology_monthly_series,truth_monthly_series)[0])))\n",
    "        f.write(\"recall - Persistence    {}\\n\".format(np.mean(recall(persist_monthly_series,truth_monthly_series)[0])))\n",
    "    f.write(\"*******************     Lead time 6     *******************\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################################################\n",
      "############   start prediction with climatology  ############\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:87: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################################################\n",
      "############   start prediction with persistence  ############\n",
      "##############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:84: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################################################\n",
      "########   start prediction with linear regression   #########\n",
      "##############################################################\n",
      "#################################################################################\n",
      "############   evaluation and statistical matrix for the entire year ############\n",
      "#################################################################################\n",
      "*******************     Lead time 0     *******************\n",
      "Total precision with testing data - ConvLSTM\n",
      "0.6626360787901849\n",
      "Total precision with testing data - Climatology\n",
      "0.4079385856750705\n",
      "Total precision with testing data - Persistence\n",
      "0.690738761572815\n",
      "*******************     Lead time 1     *******************\n",
      "Total precision with testing data - ConvLSTM\n",
      "0.6107196257319342\n",
      "Total precision with testing data - Persistence\n",
      "0.6356268444470853\n",
      "*******************     Lead time 2     *******************\n",
      "Total precision with testing data - ConvLSTM\n",
      "0.579523479629473\n",
      "Total precision with testing data - Persistence\n",
      "0.6105407500493609\n",
      "*******************     Lead time 3     *******************\n",
      "Total precision with testing data - ConvLSTM\n",
      "0.5556280316807053\n",
      "Total precision with testing data - Persistence\n",
      "0.5833019876273877\n",
      "*******************     Lead time 4     *******************\n",
      "Total precision with testing data - ConvLSTM\n",
      "0.5321481099096353\n",
      "Total precision with testing data - Persistence\n",
      "0.553914255338053\n",
      "*******************     Lead time 5     *******************\n",
      "Total precision with testing data - ConvLSTM\n",
      "0.5133675059459688\n",
      "Total precision with testing data - Persistence\n",
      "0.5249623371537447\n",
      "##############################################################\n",
      "############           create a txt file          ############\n",
      "##############################################################\n",
      "*******************    1     *******************\n",
      "precision - ConvLSTM       0.9026181601166431\n",
      "precision - Climatology    0.5559542418518143\n",
      "precision - Persistence    0.9267649426825348\n",
      "*******************    2     *******************\n",
      "precision - ConvLSTM       0.9191504507078476\n",
      "precision - Climatology    0.6083904361457534\n",
      "precision - Persistence    0.9307656748230683\n",
      "*******************    3     *******************\n",
      "precision - ConvLSTM       0.9389789991144883\n",
      "precision - Climatology    0.625218252792524\n",
      "precision - Persistence    0.9461523115764381\n",
      "*******************    4     *******************\n",
      "precision - ConvLSTM       0.931422338150755\n",
      "precision - Climatology    0.6066702234474803\n",
      "precision - Persistence    0.9246785425436064\n",
      "*******************    5     *******************\n",
      "precision - ConvLSTM       0.8758113133167318\n",
      "precision - Climatology    0.4990962228132416\n",
      "precision - Persistence    0.8529426820551145\n",
      "*******************    6     *******************\n",
      "precision - ConvLSTM       0.8264382811201926\n",
      "precision - Climatology    0.33860928286279546\n",
      "precision - Persistence    0.7542750415249005\n",
      "*******************    7     *******************\n",
      "precision - ConvLSTM       0.5637202053127027\n",
      "precision - Climatology    0.25354943889363424\n",
      "precision - Persistence    0.53082042568696\n",
      "*******************    8     *******************\n",
      "precision - ConvLSTM       0.25508710795141093\n",
      "precision - Climatology    0.25\n",
      "precision - Persistence    0.2554656667257474\n",
      "*******************    9     *******************\n",
      "precision - ConvLSTM       0.2326843886249756\n",
      "precision - Climatology    0.23414179104477612\n",
      "precision - Persistence    0.23465289256198346\n",
      "*******************    10     *******************\n",
      "precision - ConvLSTM       0.24895253003603335\n",
      "precision - Climatology    0.2184269005947958\n",
      "precision - Persistence    0.24059244560980797\n",
      "*******************    11     *******************\n",
      "precision - ConvLSTM       0.44714826265214447\n",
      "precision - Climatology    0.24949192107359747\n",
      "precision - Persistence    0.7659697420634921\n",
      "*******************    12     *******************\n",
      "precision - ConvLSTM       0.8096209083782928\n",
      "precision - Climatology    0.45571431658043265\n",
      "precision - Persistence    0.9257847710201259\n"
     ]
    }
   ],
   "source": [
    "    #################################################################################\n",
    "    ########              performance evaluation with precision              ########\n",
    "    #################################################################################\n",
    "    sequence_len, height, width = sic_exp_norm.shape\n",
    "    print('##############################################################')\n",
    "    print('############   start prediction with climatology  ############')\n",
    "    print('##############################################################')\n",
    "    # compute climatology\n",
    "    climatology = np.zeros((48, height, width),dtype=float)\n",
    "    for i in range(48):\n",
    "        climatology[i,:,:] = np.mean(sic_exp_norm[i::48,:,:],axis=0)\n",
    "    # repeat this climatology and calculate the RMSE\n",
    "    climatology_seq = np.tile(climatology,(test_year,1,1))\n",
    "    # turn into binary\n",
    "    climatology_denorm = np.zeros(climatology_seq.shape, dtype=float)\n",
    "    for i in np.arange(height):\n",
    "        climatology_denorm[:,i,:] = climatology_seq[:,i,:] / dx[i+12] * dx[35]    \n",
    "    climatology_denorm[climatology_denorm <= criterion_0] = 0\n",
    "    climatology_denorm[climatology_denorm > criterion_0] = 1\n",
    "    # turn matrix into int\n",
    "    climatology_denorm = climatology_denorm.astype(int)\n",
    "    \n",
    "    prec_climatology, _ = precision(climatology_denorm, sic_exp_bin[-test_year*12*4:,:,:])\n",
    "    print('##############################################################')\n",
    "    print('############   start prediction with persistence  ############')\n",
    "    print('##############################################################')\n",
    "    prec_persist_0, _ = precision(sic_exp_bin[-test_year*12*4-1:-1,:,:],sic_exp_bin[-test_year*12*4:,:,:])\n",
    "    prec_persist_1, _ = precision(sic_exp_bin[-test_year*12*4-1:-2,:,:],sic_exp_bin[-test_year*12*4+1:,:,:])\n",
    "    prec_persist_2, _ = precision(sic_exp_bin[-test_year*12*4-1:-3,:,:],sic_exp_bin[-test_year*12*4+2:,:,:])\n",
    "    prec_persist_3, _ = precision(sic_exp_bin[-test_year*12*4-1:-4,:,:],sic_exp_bin[-test_year*12*4+3:,:,:])\n",
    "    prec_persist_4, _ = precision(sic_exp_bin[-test_year*12*4-1:-5,:,:],sic_exp_bin[-test_year*12*4+4:,:,:])\n",
    "    prec_persist_5, _ = precision(sic_exp_bin[-test_year*12*4-1:-6,:,:],sic_exp_bin[-test_year*12*4+5:,:,:])\n",
    "    print('##############################################################')\n",
    "    print('########   start prediction with linear regression   #########')\n",
    "    print('##############################################################')\n",
    "    \n",
    "    print('#################################################################################')\n",
    "    print('############   evaluation and statistical matrix for the entire year ############')\n",
    "    print('#################################################################################')\n",
    "    prec_ConvLSTM_0, _ = precision(lead_pred_sic_bin[:,0,:,:],sic_exp_bin[-test_year*12*4:,:,:])\n",
    "    prec_ConvLSTM_1, _ = precision(lead_pred_sic_bin[:-1,1,:,:],sic_exp_bin[-test_year*12*4+1:,:,:])\n",
    "    prec_ConvLSTM_2, _ = precision(lead_pred_sic_bin[:-2,2,:,:],sic_exp_bin[-test_year*12*4+2:,:,:])\n",
    "    prec_ConvLSTM_3, _ = precision(lead_pred_sic_bin[:-3,3,:,:],sic_exp_bin[-test_year*12*4+3:,:,:])\n",
    "    prec_ConvLSTM_4, _ = precision(lead_pred_sic_bin[:-4,4,:,:],sic_exp_bin[-test_year*12*4+4:,:,:])\n",
    "    prec_ConvLSTM_5, _ = precision(lead_pred_sic_bin[:-5,5,:,:],sic_exp_bin[-test_year*12*4+5:,:,:])\n",
    "    print(\"*******************     Lead time 0     *******************\")\n",
    "    print(\"Total precision with testing data - ConvLSTM\")\n",
    "    print(np.mean(prec_ConvLSTM_0))\n",
    "    print(\"Total precision with testing data - Climatology\")\n",
    "    print(np.mean(prec_climatology))\n",
    "    print(\"Total precision with testing data - Persistence\")\n",
    "    print(np.mean(prec_persist_0))\n",
    "    print(\"*******************     Lead time 1     *******************\")\n",
    "    print(\"Total precision with testing data - ConvLSTM\")\n",
    "    print(np.mean(prec_ConvLSTM_1))\n",
    "    print(\"Total precision with testing data - Persistence\")\n",
    "    print(np.mean(prec_persist_1))\n",
    "    print(\"*******************     Lead time 2     *******************\")\n",
    "    print(\"Total precision with testing data - ConvLSTM\")\n",
    "    print(np.mean(prec_ConvLSTM_2))\n",
    "    print(\"Total precision with testing data - Persistence\")\n",
    "    print(np.mean(prec_persist_2))\n",
    "    print(\"*******************     Lead time 3     *******************\")\n",
    "    print(\"Total precision with testing data - ConvLSTM\")\n",
    "    print(np.mean(prec_ConvLSTM_3))\n",
    "    print(\"Total precision with testing data - Persistence\")\n",
    "    print(np.mean(prec_persist_3))\n",
    "    print(\"*******************     Lead time 4     *******************\")\n",
    "    print(\"Total precision with testing data - ConvLSTM\")\n",
    "    print(np.mean(prec_ConvLSTM_4))\n",
    "    print(\"Total precision with testing data - Persistence\")\n",
    "    print(np.mean(prec_persist_4))\n",
    "    print(\"*******************     Lead time 5     *******************\")\n",
    "    print(\"Total precision with testing data - ConvLSTM\")\n",
    "    print(np.mean(prec_ConvLSTM_5))\n",
    "    print(\"Total precision with testing data - Persistence\")\n",
    "    print(np.mean(prec_persist_5))\n",
    "    print('##############################################################')\n",
    "    print('############           create a txt file          ############')\n",
    "    print('##############################################################')\n",
    "    f = open(os.path.join(output_path,\"report_precision_pred_bin.txt\"),\"w+\")\n",
    "    f.write(\"############   evaluation of prediction   ############\\n\")\n",
    "    f.write(\"############   Total precision with testing data   ############\\n\")\n",
    "    f.write(\"*******************     Lead time 0     *******************\\n\")\n",
    "    f.write(\"precision - ConvLSTM       {}\\n\".format(np.mean(prec_ConvLSTM_0)))\n",
    "    f.write(\"precision - Climatology    {}\\n\".format(np.mean(prec_climatology)))\n",
    "    f.write(\"precision - Persistence    {}\\n\".format(np.mean(prec_persist_0)))\n",
    "    f.write(\"*******************     Lead time 1     *******************\\n\")\n",
    "    f.write(\"precision - ConvLSTM       {}\\n\".format(np.mean(prec_ConvLSTM_1)))\n",
    "    f.write(\"precision - Persistence    {}\\n\".format(np.mean(prec_persist_1)))\n",
    "    f.write(\"*******************     Lead time 2     *******************\\n\")\n",
    "    f.write(\"precision - ConvLSTM       {}\\n\".format(np.mean(prec_ConvLSTM_2)))\n",
    "    f.write(\"precision - Persistence    {}\\n\".format(np.mean(prec_persist_2)))\n",
    "    f.write(\"*******************     Lead time 3     *******************\\n\")\n",
    "    f.write(\"precision - ConvLSTM       {}\\n\".format(np.mean(prec_ConvLSTM_3)))\n",
    "    f.write(\"precision - Persistence    {}\\n\".format(np.mean(prec_persist_3)))\n",
    "    f.write(\"*******************     Lead time 4     *******************\\n\")\n",
    "    f.write(\"precision - ConvLSTM       {}\\n\".format(np.mean(prec_ConvLSTM_4)))\n",
    "    f.write(\"precision - Persistence    {}\\n\".format(np.mean(prec_persist_4)))\n",
    "    f.write(\"*******************     Lead time 5     *******************\\n\")\n",
    "    f.write(\"precision - ConvLSTM       {}\\n\".format(np.mean(prec_ConvLSTM_5)))\n",
    "    f.write(\"precision - Persistence    {}\\n\".format(np.mean(prec_persist_5)))\n",
    "    f.write(\"-\\n\")\n",
    "    f.write(\"-\\n\")\n",
    "    f.write(\"-\\n\")\n",
    "    f.write(\"############   Monthly accuracy with testing data   ############\\n\")\n",
    "    f.write(\"*******************     Lead time 0     *******************\\n\")\n",
    "    for i in np.arange(1,13,1):\n",
    "        ConvLSTM_monthly_series = np.zeros((test_year*4,height, width),dtype=float)\n",
    "        persist_monthly_series = np.zeros((test_year*4,height, width),dtype=float)\n",
    "        climatology_monthly_series = np.zeros((test_year*4,height, width),dtype=float)\n",
    "        truth_monthly_series = np.zeros((test_year*4,height, width),dtype=float)\n",
    "        for j in np.arange(4):\n",
    "            ConvLSTM_monthly_series[j::4,:,:] = lead_pred_sic_bin[(i-1)*4+j::48,0,:,:]\n",
    "            persist_monthly_series[j::4,:,:] = sic_exp_bin[-test_year*12*4-1+(i-1)*4+j:-1:48,:,:]\n",
    "            climatology_monthly_series[j::4,:,:] = climatology_denorm[(i-1)*4+j::48,:,:]\n",
    "            truth_monthly_series[j::4,:,:] = sic_exp_bin[-test_year*12*4+(i-1)*4+j::48,:,:]\n",
    "        print(\"*******************    {}     *******************\".format(i))\n",
    "        print(\"precision - ConvLSTM       {}\".format(np.mean(precision(ConvLSTM_monthly_series,truth_monthly_series)[0])))\n",
    "        print(\"precision - Climatology    {}\".format(np.mean(precision(climatology_monthly_series,truth_monthly_series)[0])))\n",
    "        print(\"precision - Persistence    {}\".format(np.mean(precision(persist_monthly_series,truth_monthly_series)[0])))\n",
    "        f.write(\"*******************    {}     *******************\\n\".format(i))\n",
    "        f.write(\"precision - ConvLSTM       {}\\n\".format(np.mean(precision(ConvLSTM_monthly_series,truth_monthly_series)[0])))\n",
    "        f.write(\"precision - Climatology    {}\\n\".format(np.mean(precision(climatology_monthly_series,truth_monthly_series)[0])))\n",
    "        f.write(\"precision - Persistence    {}\\n\".format(np.mean(precision(persist_monthly_series,truth_monthly_series)[0])))\n",
    "    f.write(\"*******************     Lead time 6     *******************\\n\")\n",
    "    f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
